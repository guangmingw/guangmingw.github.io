<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>Guangming Wang | The Shanghai Jiao Tong University</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/jpg" href="./imgs/sjtu_icon.png">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>


    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center"><name>Guangming Wang</name></p>
                  <p align="justify">I am a Ph.D. student (2018.09-) in the <a href="http://irmv.sjtu.edu.cn/"> Intelligent Robotics and Machine Vision (IRMV) Lab</a>
                    at <a href="http://en.sjtu.edu.cn/">the Shanghai Jiao Tong University</a> advised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=q6AY9XsAAAAJ">Hesheng Wang</a>. I obtained my B.Eng. degree (2014.09-2018.06)
                    from <a href="http://en.csu.edu.cn/">the Central South University</a>, where I was in the Physics Sublimation Honor Class.

                    <p align="justify">From 2021.09, I remotely work with Prof. <a href="https://msc.berkeley.edu/people/tomizuka.html"> Masayoshi Tomizuka</a> from <a href="https://www.berkeley.edu/">UC Berkeley</a>.
                    
                    <p align="justify">Currently, I am a visiting researcher in the <a href="http://www.cvg.ethz.ch/index.php"> Computer Vision and Geometry Group (CVG)</a>, <a href="https://ethz.ch/en.html">ETH Zurich</a>, advised by <a href="https://people.inf.ethz.ch/pomarc/"> Prof. Marc Pollefeys</a>.
                    <!-- </br></br>
                    In my D.Phil study, I interned at the Augumented Reality team of <a href="http://www.a9.com/">Amazon</a> (Palo Alto, CA).
                    In my M.Phil study, I interned at <a href="https://www.astri.org/">Hong Kong Applied Science and Technology Research Institute</a>.
                    In my undergraduate study, I was an exchange student at <a href="http://www.upv.es/">Universitat Politècnica de València</a> (Valencia, Spain). -->

	            </br>
                </p><p align="center">
                    <a href="mailto:wangguangming@sjtu.edu.cn">Email</a> /
                    <a href="https://scholar.google.com/citations?hl=en&user=GGHfHSIAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/guangmingw"> Github </a>
                </p>
              </td>
			  <td align="right"> <img class="hp-photo" src="./imgs/photo.jpg" style="width: 240;"></td></tr>
            </tbody>
          </table>

    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Research</heading>
              <p align="justify">My research interests include computer vision for autonomous robots and robot learning, specifically on topics as:</p>
              <ul>
                <li><p align="justify">Robot perception: depth estimation, optical/scene flow estimation, 3D semantic segmentation, 3D object detection/tracking.</p></li>
                <li><p align="justify">Robot localization: visual/LiDAR odometry, 2D/3D registration, robot relocalization.</p></li>
                <li><p align="justify">Robot mapping: implicit neural field based mapping.</p></li>
                <li><p align="justify">Robot learning: reinforcement learning for manipulator tasks.</p></li>
              </ul>

		   </br>

           
		   <!-- <font color="red"><strong>Openings:</strong> </font></br></br>

            <font color="red">Several fully funded PhD positions and Research Assistantships are available now. Welcome to drop me an email with your CV and transcripts. </font> -->
		   </td></tr>
       </tbody>
    </table>

    <!--SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>News</heading>
            <!-- <p style="font-size:13px"> <strong>[2020.11.24]</strong>  Our latest paper SpinNet is on <a href="https://arxiv.org/abs/2011.12149">arXiv</a>.</p>

            <p style="font-size:13px"> <strong>[2020.10.12]</strong>  Our paper GRF is on <a href="http://arxiv.org/abs/2010.04595">arXiv</a>.</p>

            <p style="font-size:13px"> <strong>[2020.09.10]</strong> Pass my D.Phil thesis,
                examined by Profs. <a href="https://mpawankumar.info/">M. Pawan Kumar </a> and <a href="https://www.doc.ic.ac.uk/~ajd/">Andrew Davison</a>.</p>

            <p style="font-size:13px"> <strong>[2020.03.08]</strong> Invited to present our <a href="https://arxiv.org/abs/1911.11236">RandLA-Net</a>
                and <a href="https://arxiv.org/abs/1906.01140">3D-BoNet</a> at Shenlan.
                Here are the <a href="https://www.shenlanxueyuan.com/open/course/53">Video</a>
                and <a href="https://www.dropbox.com/s/80w91bqzfdl5vow/%E6%B7%B1%E8%93%9D%E5%AD%A6%E9%99%A2%E5%85%AC%E5%BC%80%E8%AF%BE_20200308.pdf?dl=0">Slides</a>. </p>
            <p style="font-size:13px"> <strong>[2020.02.27]</strong> One co-authored <a href="https://arxiv.org/abs/1911.11236">paper</a> for 3D semantic segmentation is accepted by <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>. </p>
            <p style="font-size:13px"> <strong>[2019.10.24]</strong> Pass my
                <a href="https://www.ox.ac.uk/students/academic/guidance/graduate/research/status/DPhil?wssl=1">D.Phil confirmation</a>, examined by Profs.
                <a href="https://scholar.google.co.uk/citations?user=UZ5wscMAAAAJ&hl=en">Andrew Zisserman</a> and
                <a href="https://www.cs.ox.ac.uk/people/alessandro.abate/home.html">Alessandro Abate</a>.</p>-->
                <p style="font-size:13px"> <strong>[2022.12.08]</strong> Got Shanghai Jiao Tong University <a href="https://www.gs.sjtu.edu.cn/info/1115/9386.htm">"Academic Star" Nomination Award</a> <font color="red"><strong> (Top 0.2%)</strong></font>! </p> / News: (<a href="https://mp.weixin.qq.com/s/eL7uOsRdEbMJXD_SskyGGg"><font color="red">Shanghai Jiao Tong University</font></a>, <a href="https://mp.weixin.qq.com/s/ACEisGNbhbIipRIRK2fkXw"><font color="red">School of Electronic Information and Electrical Engineering</font></a>) </p>
               
                <p style="font-size:13px"> <strong>[2022.11.19]</strong> One co-first-author paper on the learning of LiDAR odometry with Transformer is accepted by top conference <a href="https://aaai.org/Conferences/AAAI-23/">AAAI 2023</a>! </p>
                
                <p style="font-size:13px"> <strong>[2022.10.11]</strong> One co-first-author <a href="https://ieeexplore.ieee.org/abstract/document/9924173">paper</a> for the unsupervised learning of depth and pose is accepted by top journal TCSVT 2022<font color="red"><strong> (IF=5.859)</strong></font>! </p>
                
                <p style="font-size:13px"> <strong>[2022.09.26]</strong> Got National Scholarships for Doctoral Students <font color="red"><strong> again (Top 2%)</strong></font>! </p>
                <p style="font-size:13px"> <strong>[2022.09.13]</strong> One co-first-author <a href="https://arxiv.org/abs/2111.02135">paper</a> for the efficient 3D deep LiDAR odometry is accepted by top journal TPAMI 2022<font color="red"><strong> (IF=24.314)</strong></font>! </p>
                <p style="font-size:13px"> <strong>[2022.08.23]</strong> One co-first-author <a href="https://ieeexplore.ieee.org/abstract/document/9882014">paper</a> for the robot manipulator learning is accepted by top journal TNNLS 2022<font color="red"><strong> (IF=14.255)</strong></font>! </p>
                <p style="font-size:13px"> <strong>[2022.07.03]</strong> One co-first-author <a href="https://link.springer.com/chapter/10.1007/978-3-031-19827-4_3">paper</a> for the 3D scene flow estimation is accepted by top conference <a href="https://eccv2022.ecva.net/">ECCV 2022</a>! </p>
                
                <p style="font-size:13px"> <strong>[2022.06.23]</strong> Join <a href="http://www.cvg.ethz.ch/index.php"> Computer Vision and Geometry Group (CVG)</a>, <a href="https://ethz.ch/en.html">ETH Zurich</a> as a visiting researcher! </p>
                <p style="font-size:13px"> <strong>[2022.06.09]</strong> One co-first-author <a href="https://ieeexplore.ieee.org/abstract/document/9810863">paper</a> for unsupervised learning of optical flow estimation is accepted by top journal T-ITS 2022<font color="red"><strong> (IF=9.551)</strong></font>! </p>
                <p style="font-size:13px"> <strong>[2022.06.07]</strong> One co-first-author <a href="https://ieeexplore.ieee.org/document/9802736">paper</a> for 3D human pose estimation is accepted by TCSD 2022<font color="red"><strong> (IF=4.546)</strong></font>! </p>
                <p style="font-size:13px"> <strong>[2022.02.01]</strong> One co-first-author <a href="https://ieeexplore.ieee.org/abstract/document/9811945">paper</a> for online calibration of camera and LiDAR is accepted by top conference <a href="https://www.icra2022.org/">ICRA 2022</a>! </p>
            <p style="font-size:13px"> <strong>[2021.11.18]</strong> One first-authored <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/aisy.202100197">paper</a> for 3D scene flow is accepted by Advanced Intelligent Systems 2021 <font color="red"><strong>(AIS, IF=7.298)</strong></font>！</p> 
            <p style="font-size:13px"> <strong>[2021.10.27]</strong> One first-authored <a href="https://ieeexplore.ieee.org/abstract/document/9606554/">paper</a> for 3D semantic segmentation is accepted by T-cybe 2021 <font color="red"><strong>(IF=19.118)</strong></font>！</p> 
            <p style="font-size:13px"> <strong>[2021.09.24]</strong> Got National Scholarships for Doctoral Students <font color="red"><strong> (Top 2%)</strong></font>! / News: (<a href="https://mp.weixin.qq.com/s/cvWT4Md0G5dIUf_AXc79UQ"><font color="red">Department of
                Automation</font></a>, <a href="https://mp.weixin.qq.com/s/VcBSquX6fakbOjDJ-g5zXg"><font color="red">School of Electronic Information and Electrical Engineering</font></a>) </p>
            <p style="font-size:13px"> <strong>[2021.08.09]</strong> One first-authored <a href="https://ieeexplore.ieee.org/abstract/document/9522122/">paper</a> for 3D action recognition and 3D semantic segmentation is accepted by TIM 2021 <font color="red"><strong>(IF=5.332)</strong></font>！</p> 
            <p style="font-size:13px"> <strong>[2021.05.18]</strong> One co-first-author <a href="https://ieeexplore.ieee.org/abstract/document/9435105/">paper</a> for 3D scene flow is accepted by top journal TIP 2021 <font color="red"><strong>(IF=11.041)</strong></font>! </p>
            <p style="font-size:13px"> <strong>[2021.03.01]</strong> One co-first-author <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_PWCLO-Net_Deep_LiDAR_Odometry_in_3D_Point_Clouds_Using_Hierarchical_CVPR_2021_paper.html">paper</a> for 3D LiDAR odometry is accepted by top conference <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>! </p>
            <p style="font-size:13px"> <strong>[2021.03.01]</strong> One co-first-author <a href="https://ieeexplore.ieee.org/abstract/document/9561572">paper</a> for 3D scene flow is accepted by top conference <a href="http://www.icra2021.org/">ICRA 2021</a>! </p>
            <!-- <p style="font-size:13px"> <strong>[2020.11.16]</strong> One co-first-author paper is submitted to CVPR 2021! </p> -->
            <!-- <p style="font-size:13px"> <strong>[2020.10.31]</strong> One co-first-author paper and one second-author paper are submitted to ICRA 2021! </p> -->
          </td>
       </tr></tbody>
    </table>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading>Publications / Preprints</heading>
          </td>
          </tr></tbody>
    </table>

        <!--SECTION 5 -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>

        <td width="20%"><img src="./imgs/20_arxiv_DDPGwB.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://ieeexplore.ieee.org/abstract/document/9882014">
	            <papertitle>Learning of Long-Horizon Sparse-Reward Robotic Manipulator Tasks With Base Controllers</papertitle></a>
                <br><strong>G. Wang*</strong>, M. Xin*, Z. Liu, and H. Wang
                <br>
                <em>IEEE Transactions on Neural Networks and Learning Systems (T-NNLS)</em>, 2022 <font color="red"><strong>(IF=19.118)</strong></font><br>
		        <a href="https://arxiv.org/abs/2011.12105">arXiv</a>/
		        <a href="https://ieeexplore.ieee.org/abstract/document/9882014">IEEE Xplore</a>/
                <a href="https://github.com/IRMVLab/BCLearning"><font color="red">Code</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=BCLearning&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="100px" height="20px"></iframe>

                 <br>(* indicates equal contributions)
                <br>
                <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
                </p><p></p>
			    <p align="justify" style="font-size:13px">We introduce a method of learning challenging sparse-reward tasks utilizing existing controllers. Compared to previous works of learning from demonstrations, our method improves sample efficiency by orders of magnitude and can learn online in a safe manner.  </p>
            </td>
        </tr>

    
                <td width="20%"><img src="./imgs/efficient_odometry.png" alt="PontTuset" width="180" style="border-style: none"></td>
                <td width="80%" valign="top">
                    <p><a href="https://ieeexplore.ieee.org/abstract/document/9893384">
                    <papertitle> Efficient 3D Deep LiDAR Odometry</papertitle></a>
                    <br><strong>G. Wang*</strong>, X. Wu*, S. Jiang, Z. Liu, and H. Wang
                    <br>
                    <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2022 <font color="red"><strong>(IF=24.314)</strong></font><br>
                    <a href="https://arxiv.org/abs/2111.02135"> arXiv</a>/
                    <a href="https://ieeexplore.ieee.org/abstract/document/9893384">IEEE Xplore</a>/
                <a href="https://github.com/IRMVLab/EfficientLO-Net"><font color="red">Code</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=EfficientLO-Net&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                    
                    <br>(* indicates equal contributions)
                </p><p></p>
                    <p align="justify" style="font-size:13px">We propose a new efficient 3D point cloud learning method, which is specially designed for the frame-by-frame processing task of real-time perception and localization of robots. It can accelerate the deep LiDAR odometry of our previous CVPR to real-time, while improving the accuracy.</p>
                </td>
            </tr>
            
            


            <td width="20%"><img src="./imgs/ECCV.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://link.springer.com/chapter/10.1007/978-3-031-19827-4_3">
                 <papertitle>What Matters for 3D Scene Flow Network </papertitle></a>
                 <br> <strong>G. Wang*</strong>, Y. Hu, Z. Liu, Y. Zhou, W. Zhan, M. Tomizuka, and H. Wang<br>

                 <a href="https://eccv2022.ecva.net/"> European Conference on Computer Vision  (ECCV), 2022</a><br>
                 <a href="https://arxiv.org/abs/2012.00972">arXiv</a>/
                 <a href="https://link.springer.com/chapter/10.1007/978-3-031-19827-4_3">ECCV 2022</a>/
                 <a href="https://github.com/IRMVLab/3DFlow"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=3DFlow&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                 <!-- <a href="https://arxiv.org/abs/2012.00972">arXiv</a> / -->
                 <!-- <a href="https://github.com/QingyongHu/SpinNet"><font color="red">Code</font></a> -->
                 <!-- <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SpinNet&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe> -->
                 <br>(* indicates equal contributions)
                 <p align="justify" style="font-size:13px">We introduce a novel flow embedding layer with all-to-all mechanism and reverse verification mechanism. Besides,
                    we investigate and compare several design choices in key components of the 3D scene flow network and achieve SOTA performance.
                 </p>
                <p></p>
            </td>
        </tr>



        <td width="20%"><img src="./imgs/20_arxiv_3d_odometry.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_PWCLO-Net_Deep_LiDAR_Odometry_in_3D_Point_Clouds_Using_Hierarchical_CVPR_2021_paper.html">
                 <papertitle>PWCLO-Net: Deep LiDAR Odometry in 3D Point Clouds Using Hierarchical Embedding Mask Optimization</papertitle></a>
                 <br> <strong>G. Wang*</strong>, X. Wu*, Z. Liu, and H. Wang<br>

                 <a href="http://cvpr2021.thecvf.com/"> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021</a>
                 <a href="https://arxiv.org/abs/2012.00972">arXiv</a>/
                 <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_PWCLO-Net_Deep_LiDAR_Odometry_in_3D_Point_Clouds_Using_Hierarchical_CVPR_2021_paper.html">CVPR 2021</a>/
                 <a href="https://github.com/IRMVLab/PWCLONet"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=PWCLONet&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                 <!-- <a href="https://arxiv.org/abs/2012.00972">arXiv</a> / -->
                 <!-- <a href="https://github.com/QingyongHu/SpinNet"><font color="red">Code</font></a> -->
                 <!-- <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SpinNet&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe> -->
                 <br>(* indicates equal contributions)
                 <p align="justify" style="font-size:13px">We introduce a novel 3D point cloud learning model for deep LiDAR odometry, named PWCLO-Net, using hierarchical embedding mask optimization. It outperforms all recent learning-based methods and outperforms the geometry-based approach, LOAM with mapping optimization, on most sequences of the KITTI odometry dataset.
                 </p>
                <p></p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/detection.png" alt="PontTuset" width="180" style="border-style: none"></td>
        <td width="80%" valign="top">
            <p><a href="https://arxiv.org/abs/2209.07419">
            <papertitle> FFPA-Net: Efficient Feature Fusion with Projection Awareness for 3D Object Detection
                </papertitle></a>
            <br>C. Jiang, <strong>G. Wang</strong>, J. Wu, Y. Miao, and H. Wang
            <br>
            <a href="https://arxiv.org/abs/2209.07419"> arXiv</a>
            <br>
            <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
            </p><p></p>
            <p align="justify" style="font-size:13px">We propose an efficient feature fusion framework with projection awareness for 3D
                Object Detection.</p>
        </td>
    </tr>

        <td width="20%"><img src="./imgs/tracking.png" alt="PontTuset" width="180" style="border-style: none"></td>
        <td width="80%" valign="top">
            <p><a href="https://arxiv.org/abs/2203.16268">
            <papertitle> Interactive Multi-scale Fusion of 2D and 3D
                Features for Multi-object Tracking
                </papertitle></a>
            <br><strong>G. Wang</strong>, C. Peng, J. Zhang, and H. Wang
            <br>
            <a href="https://arxiv.org/abs/2203.16268"> arXiv</a>
            <br>
            <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
            </p><p></p>
            <p align="justify" style="font-size:13px">We propose an interactive feature fusion between multi-scale features of images and point clouds. Besides, we explore the effectiveness of pre-training on each single modality and fine-tuning on the fusion-based model.</p>
        </td>
    </tr>


        <td width="20%"><img src="./imgs/DetFlowTrack.png" alt="PontTuset" width="180" style="border-style: none"></td>
        <td width="80%" valign="top">
            <p><a href="https://arxiv.org/abs/2203.02157">
            <papertitle> DetFlowTrack: 3D Multi-object Tracking based on Simultaneous Optimization of Object Detection and Scene Flow Estimation
                </papertitle></a>
            <br>Y. Shen, <strong>G. Wang</strong>, and H. Wang
            <br>
            <a href="https://arxiv.org/abs/2203.02157"> arXiv</a>
            <br>
            <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
            </p><p></p>
            <p align="justify" style="font-size:13px">We propose a new joint learning method for 3D object detection and 3D multi-object tracking based on 3D scene flow. </p>
        </td>
    </tr>


        <td width="20%"><img src="./imgs/Residual_Scene_Flow.png" alt="PontTuset" width="180" style="border-style: none"></td>
        <td width="80%" valign="top">
            <p><a href="https://arxiv.org/abs/2109.04685">
            <papertitle> Residual 3D Scene Flow Learning with Context-Aware Feature Extraction</papertitle></a>
            <br><strong>G. Wang*</strong>, Y. Hu*, X. Wu, and H. Wang
            <br>
            <em>IEEE Transactions on Instrumentation and Measurement (TIM)</em>, 2022 <font color="red"><strong>(IF=5.332)</strong></font><br>
            <a href="https://arxiv.org/abs/2109.04685"> arXiv</a>/
            <a href="https://ieeexplore.ieee.org/abstract/document/9754543"> IEEE Xplore</a>/
            <a href="https://github.com/IRMVLab/CARFlow"><font color="red">Code</font></a>
            <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=CARFlow&type=star&count=true&size=small"
            frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
            <br>(* indicates equal contributions)
            <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
            </p><p></p>
            <p align="justify" style="font-size:13px">We propose a novel context-aware set conv layer to cope with repetitive patterns in the learning of 3D scene flow. We also propose an explicit residual flow learning structure in the residual flow refinement layer to cope with long-distance movement. </p>
        </td>
    </tr>


    <td width="20%"><img src="./imgs/mul_posenet.png" alt="PontTuset" width="180" style="border-style: none"></td>
    <td width="80%" valign="top">
        <p><a href="https://ieeexplore.ieee.org/abstract/document/9924173">
        <papertitle> 3D Hierarchical Refinement and Augmentation for Unsupervised Learning of Depth and Pose from Monocular Video</papertitle></a>
        <br><strong>G. Wang</strong>, J. Zhong, S. Zhao, W. Wu, Z. Liu, and H. Wang
        <br>
            <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em>, 2022 <font color="red"><strong>(IF=5.859)</strong></font><br>
            <a href="https://arxiv.org/abs/2112.03045"> arXiv</a>/
            <a href="https://ieeexplore.ieee.org/abstract/document/9924173"> IEEE Xplore</a>/
            <a href="https://github.com/IRMVLab/HRANet"><font color="red">Code</font></a>
            <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=HRANet&type=star&count=true&size=small"
            frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
            <br>(* indicates equal contributions)
        <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
        </p><p></p>
        <p align="justify" style="font-size:13px">We propose a novel unsupervised training framework of depth and pose with 3D hierarchical refinement and augmentation using explicit 3D geometry. </p>
    </td>
</tr>


<td width="20%"><img src="./imgs/fusionnet.png" alt="PontTuset" width="180" style="border-style: none"></td>
<td width="80%" valign="top">
    <p><a href="https://ieeexplore.ieee.org/abstract/document/9811945">
    <papertitle>FusionNet: Coarse-to-Fine Extrinsic Calibration Network of LiDAR and Camera with Hierarchical Point-pixel Fusion</papertitle></a>
    <br><strong>G. Wang*</strong>, J. Qiu*, Y. Guo*, and H. Wang<br>
    
    <a href="https://www.icra2022.org/">International Conference on Robotics and Automation (ICRA), Xi'an, China, 2021. </a>
    <br>
    <a href="https://ieeexplore.ieee.org/abstract/document/9811945"> IEEE Xplore</a>
    <br>(* indicates equal contributions)

    </p><p></p>
    <p align="justify" style="font-size:13px">We propose Fusion-Net, an online and end-to-end
        solution that can automatically detect and correct the the extrinsic
        calibration matrix between LiDAR and a monocular RGB camera without any specially
        designed targets or environments. </p>
</td>
</tr>


    <td width="20%"><img src="./imgs/SFGAN.png" alt="PontTuset" width="180" style="border-style: none"></td>
    <td width="80%" valign="top">
        <p><a href="https://onlinelibrary.wiley.com/doi/full/10.1002/aisy.202100197">
        <papertitle> SFGAN: Unsupervised Generative Adversarial Learning of 3D Scene Flow from the 3D Scene Self</papertitle></a>
        <br><strong>G. Wang</strong>, C. Jiang, Z. Shen, Y. Miao, and H. Wang
        <br>
        <a href="https://onlinelibrary.wiley.com/page/journal/26404567/homepage/productinformation.html">Advanced Intelligent Systems (AIS)</a>, 2021 <font color="red"><strong>(AIS, IF=7.298)</strong></font><br>
                 <a href="https://www.authorea.com/doi/full/10.22541/au.163335790.03073492">authorea</a>/
                 <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/aisy.202100197"> Wiley Online Library</a>
        <br>
        <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
        </p><p></p>
        <p align="justify" style="font-size:13px">We utilize the generative adversarial networks (GAN) to self-learn 3D scene flow without ground truth. </p>
    </td>
</tr>


        <td width="20%"><img src="./imgs/20_arxiv_un_scene_flow.png" alt="PontTuset" width="180" style="border-style: none"></td>
        <td width="80%" valign="top">
            <p><a href="https://ieeexplore.ieee.org/abstract/document/9561572">
            <papertitle>Unsupervised Learning of Scene Flow from Monocular Camera</papertitle></a>
            <br><strong>G. Wang*</strong>, X. Tian*, R. Ding, and H. Wang<br>
            
            <a href="http://www.icra2021.org/">International Conference on Robotics and Automation (ICRA), Xi'an, China, 2021. </a>
            <br>
            <a href="https://arxiv.org/abs/2206.03673">arXiv</a>/
            <a href="https://ieeexplore.ieee.org/abstract/document/9561572"> IEEE Xplore</a>
            <br>(* indicates equal contributions)

            </p><p></p>
            <p align="justify" style="font-size:13px">We present a framework to realize the unsupervised learning of scene flow from monocular camera. </p>
        </td>
    </tr>

        <td width="20%"><img src="./imgs/20_arxiv_ASTAConv.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://ieeexplore.ieee.org/abstract/document/9522122/">
                 <papertitle>Anchor-Based Spatio-Temporal Attention 3D Convolutional Networks for Dynamic 3D Point Cloud Sequences</papertitle></a>
                 <br><strong>G. Wang</strong>, H. Liu, M. Chen, Y. Yang, Z. Liu, and H. Wang<br>
                 <em>IEEE Transactions on Instrumentation and Measurement (TIM)</em>, 2021 <font color="red"><strong>(IF=5.332)</strong></font><br>
                 <a href="https://arxiv.org/abs/2012.10860">arXiv</a>/
                 <a href="https://ieeexplore.ieee.org/abstract/document/9522122/"> IEEE Xplore</a>/
                 <a href="https://github.com/IRMVLab/ASTA3DConv"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=ASTA3DConv&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                 <br>
                  <!--<font color="red"><strong>..</strong></font><br>-->
                 <!-- <a href="http://arxiv.org/abs/2009.03137">arXiv</a> /
                 <a href="https://www.youtube.com/watch?v=IG0tTdqB3L8">Demo</a> /
                 <a href="https://github.com/QingyongHu/SensatUrban"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SensatUrban&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe> -->
                <!-- <br>(* indicates corresponding author) -->
                <!-- <br>(* indicates equal contributions) -->
                <p align="justify" style="font-size:13px">We introduce an Anchor-based Spatial-Temporal Attention Convolution operation (ASTAConv) to process dynamic 3D point cloud sequences.  It makes better use of the structured information within the local region, and learn spatial-temporal embedding features from dynamic 3D point cloud sequences.
                  </p>
                <p></p>
            </td>
        </tr>


                <td width="20%"><img src="./imgs/20_arxiv_HAFlowNet.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://ieeexplore.ieee.org/abstract/document/9435105/">
                 <papertitle>Hierarchical Attention Learning of Scene Flow in 3D Point Clouds</papertitle></a>
                 <br><strong>G. Wang*</strong>, X. Wu*, Z. Liu, and H. Wang<br>
		<em>IEEE Transactions on Image Processing (TIP)</em>, 2021 <font color="red"><strong>(IF=11.041)</strong></font><br>
		        <a href="https://arxiv.org/abs/2010.05762">arXiv</a>/
		        <a href="https://ieeexplore.ieee.org/abstract/document/9435105/">IEEE Xplore</a>/
                <a href="https://github.com/IRMVLab/HALFlow"><font color="red">Code</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=HALFlow&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                 <br>
                  <!--<font color="red"><strong>..</strong></font><br>-->
                (* indicates equal contributions)
                <p align="justify" style="font-size:13px">We introduce a novel hierarchical neural network with double attention for learning the correlation of point features in adjacent frames and refining scene flow from coarse to fine layer by layer. It has a new more-for-less hierarchical architecture. The proposed network achieves the state-of-the-art performance of 3D scene flow estimation on the FlyingThings3D and KITTI Scene Flow 2015 datasets.
                  </p>
                <p></p>
            </td>
        </tr>



    <td width="20%"><img src="./imgs/nccflow.png" alt="PontTuset" width="180" style="border-style: none"></td>
    <td width="80%" valign="top">
        <p><a href="https://arxiv.org/abs/2107.03610">
        <papertitle> NccFlow: Unsupervised Learning of Optical Flow With Non-occlusion from Geometry</papertitle></a>
        <br><strong>G. Wang*</strong>, S. Ren*, and H. Wang
        <br>
        <em>IEEE Transactions on Intelligent Transportation Systems (T-ITS)</em>, 2022 <font color="red"><strong>(IF=9.551)</strong></font><br>
        <a href="https://arxiv.org/abs/2107.03610"> arXiv</a>/
        <a href="https://github.com/IRMVLab/NccFlow"><font color="red">Code</font></a>
        <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=NccFlow&type=star&count=true&size=small"
        frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
        <br>
        (* indicates equal contributions)
        <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
        </p><p></p>
        <p align="justify" style="font-size:13px">We introduce a novel unsupervised learning method of optical flow by considering the constraints in non-occlusion regions with geometry analysis. </p>
    </td>
</tr>

    <td width="20%"><img src="./imgs/3D_human_pose.gif" alt="PontTuset" width="180" style="border-style: none"></td>
    <td width="80%" valign="top">
        <p><a href="https://arxiv.org/abs/2106.14706">
        <papertitle> Motion Projection Consistency Based 3D Human Pose Estimation with Virtual Bones from Monocular Videos</papertitle></a>
        <br><strong>G. Wang*</strong>, H. Zeng*, Z. Wang, Z. Liu, and H. Wang
        <br>
        <em>IEEE Transactions on Cognitive and Developmental Systems (TCSD)</em>, 2022 <font color="red"><strong>(IF=4.546)</strong></font><br>
        <a href="https://arxiv.org/abs/2106.14706"> arXiv</a>/
        <a href="https://ieeexplore.ieee.org/document/9802736"> IEEE Xplore</a>
        <br>(* indicates equal contributions)
        </p><p></p>
        <p align="justify" style="font-size:13px">We introduce a novel unsupervised learning method of 3D human pose by considering the loop constraints from real/virtual bones and the joint motion constraints in consecutive frames. </p>
    </td>
</tr>

<td width="20%"><img src="./imgs/arxiv_Spherical_ Interpolated.gif" alt="PontTuset" width="180" style="border-style: none"></td>
<td width="80%" valign="top">
     <p><a href="https://arxiv.org/abs/2011.13784">
     <papertitle>Spherical Interpolated Convolutional Network with Distance-Feature Density for 3D Semantic Segmentation of Point Clouds</papertitle></a>
     <br><strong>G. Wang</strong>, Y. Yang, Z. Liu, and H. Wang <br>
		<em>IEEE Transactions on Cybernetics (T-Cyb)</em>, 2021 <font color="red"><strong>(IF=19.118)</strong></font><br>
		        <a href="https://arxiv.org/abs/2011.13784">arXiv</a>/
		        <a href="https://ieeexplore.ieee.org/abstract/document/9606554/">IEEE Xplore</a>
                 <br>
     <!-- / -->
     <!-- <a href="https://github.com/alextrevithick/GRF"><font color="red">Code</font></a> -->
     <!-- <iframe src="https://ghbtns.com/github-btn.html?user=alextrevithick&repo=GRF&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe> -->
     
     <p align="justify" style="font-size:13px">We introduce  a spherical interpolated convolution operator to replace the traditional grid-shaped 3D convolution operator. It improves the accuracy and reduces the parameters of the network.
     </p>
    <p></p>
</td>
</tr>



	    <td width="20%"><img src="./imgs/DOPlearning.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                <p><a href="https://ieeexplore.ieee.org/document/9152137">
	            <papertitle>Unsupervised Learning of Depth, Optical Flow and Pose with Occlusion From 3D Geometry</papertitle></a>
                <br><strong>G. Wang</strong>, C. Zhang, H. Wang, J. Wang, Y. Wang, and X. Wang<br>
                <em>IEEE Transactions on Intelligent Transportation Systems (T-ITS)</em>, 2020 <font color="red"><strong>(IF=9.551)</strong></font><br>
		        <a href="https://arxiv.org/abs/2003.00766">arXiv</a>/
		        <a href="https://ieeexplore.ieee.org/document/9152137">IEEE Xplore</a>/
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/NXr2-AOrl_VllIs_2buLwA"><font color="red">(DeepBlue深兰科技,</font></a>
                <a href="https://ifosa.org/invited.html"><font color="red">The First International Forum on 3D Optical Sensing and Applications (iFOSA 2020),</font></a>
                <!-- <a href="https://mp.weixin.qq.com/s/C1FDPkAkmnmAZ_gvvtzBHw"><font color="red">AI科技评论,</font></a>
                <a href="https://mp.weixin.qq.com/s/wViZITtsb4j3oFtOpJI9wQ"><font color="red">将门创投,</font></a>
                <a href="https://mp.weixin.qq.com/s/S7mHrOxOwTIhDGPhu1SI4A"><font color="red">CVer,</font></a> -->
                <a href="https://mp.weixin.qq.com/s/GaS2F-2-aEQIJ-6ZLahynQ"><font color="red">计算机视觉life)</font>/</a>
                <a href="https://www.youtube.com/watch?v=Y8Up9OsZbcg"><font color="red">Video</font></a>/
                <a href="https://github.com/Yang7879/AttSets"><font color="red">Code</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=guangmingw&repo=DOPlearning&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose a method to explicitly handle occlusion, propose the less-than-mean mask, the maximum normalization, and the consistency of depth-pose and optical flow in the occlusion regions. </p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/occlusion_mask.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://ieeexplore.ieee.org/abstract/document/8793622">
	             <papertitle>Unsupervised Learning of Monocular Depth and Ego-Motion Using Multiple Masks</papertitle></a>
                 <br><strong>G. Wang</strong>, H. Wang, Y. Liu, and W. Chen
                 <br>
                 <em>International Conference on Robotics and Automation (ICRA)</em>, Montreal, Canada, 2019 
                 <br>
                 <!--<font color="red"><strong>..</strong></font><br>-->
                 <a href="https://arxiv.org/abs/2104.00431">arXiv</a>/
                 <a href="https://ieeexplore.ieee.org/abstract/document/8793622">IEEE Xplore</a> /
                 <!-- <a href="http://kaldir.vc.in.tum.de/scannet_benchmark/result_details?id=118">ScanNet Benchmark</a> / -->
                 <!-- <a href="https://www.reddit.com/r/MachineLearning/comments/bx8jhz/r_new_sota_for_3d_object_detection/">Reddit Discussion</a> / -->
                 <font color="red"> News:</font>
                 <a href="https://mp.weixin.qq.com/s/sKSb-dLRTifiwK9hcx_Vcg"><font color="red">(泡泡机器人,</font></a>
                 <!-- <a href="https://mp.weixin.qq.com/s/4GPkmTri4Vk7Xy0J8TiBNw"><font color="red">图像算法,</font></a>
                 <a href="https://mp.weixin.qq.com/s/C1FDPkAkmnmAZ_gvvtzBHw"><font color="red">AI科技评论,</font></a>
                 <a href="https://mp.weixin.qq.com/s/wViZITtsb4j3oFtOpJI9wQ"><font color="red">将门创投,</font></a>
                 <a href="https://mp.weixin.qq.com/s/S7mHrOxOwTIhDGPhu1SI4A"><font color="red">CVer,</font></a> -->
                 <a href="https://mp.weixin.qq.com/s/-IcQKYpXcs6CPme8d7Jbww"><font color="red">上海交大研究生教育)</font>/</a>
                 <a href="https://ieeexplore.ieee.org/ielx7/8780387/8793254/8793622/2533_MM.zip?tp=&arnumber=8793622"><font color="red">Video</font></a>/
                 <a href="https://github.com/guangmingw/DOPlearning"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=guangmingw&repo=DOPlearning&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
			     <p align="justify" style="font-size:13px">We propose a new unsupervised learning method of depth and ego-motion using multiple masks to handle the occlusion problem.</p>
                <p></p>
            </td>
        </tr>



	   
        </tbody>
    </table>


    <!--SECTION 6 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Talks & Academic Services</heading>
                <p style="font-size:13px"> <strong>[2022.10]</strong> Conference Talk on “What Matters for 3D Scene Flow Network,” at <a href="https://eccv2022.ecva.net/">ECCV 2022 </a> in Tel Aviv, Israel, October 23-27, 2022.</p>
                <p style="font-size:13px"> <strong>[2021.06]</strong> Conference Talk on “FusionNet: Coarse-to-Fine Extrinsic Calibration Network of LiDAR and Camera with Hierarchical Point-pixel Fusion” at  <a href="https://www.icra2022.org/">ICRA 2022</a>  in May 23-27, 2022 (Online Talk, the conference is held in Philadelphia (PA), USA).</p>
                
                <p style="font-size:13px"> <strong>[2021.06]</strong> Conference Talk on “PWCLO-Net: Deep LiDAR Odometry in 3D Point Clouds Using Hierarchical Embedding Mask Optimization” at <a href="http://cvpr2021.thecvf.com/">CVPR 2021 </a> in CVPR Virtual, June 19-25, 2021.</p>
                <p style="font-size:13px"> <strong>[2021.05]</strong> Conference Talk on “Unsupervised Learning of Scene Flow from Monocular Camera” at <a href="http://www.icra2021.org/"> ICRA 2021</a> in Xi’an, China, May 30 - June 5, 2021. </p>
                <p style="font-size:13px"> <strong>[2020.10]</strong> Invited Talk on “Unsupervised Learning of Depth, Optical Flow and Pose with Occlusion From 3D Geometry” at <a href="https://ifosa.org/invited.html"> the First International Forum on 3D Optical Sensing and Applications (iFOSA 2020)</a> held in Being, China, October 17-18, 2020.</p>
                <p style="font-size:13px"> <strong>[2019.07]</strong> Seminar Talk on “Unsupervised Learning of Monocular Depth and Ego-Motion Using Multiple Masks” at <a href="./imgs/SEEEP.mp4">Sino-European Engineering Education Platform (SEEEP) Doctoral Summer School</a> in Instituto Superior Técnico (IST), Lisbon, Portugal, July 22-25, 2019.</p>
                <p style="font-size:13px"> <strong>[2019.05]</strong> Conference Talk on “Unsupervised Learning of Monocular Depth and Ego-Motion Using Multiple Masks” at <a href="https://mp.weixin.qq.com/s/-IcQKYpXcs6CPme8d7Jbww">ICRA 2019</a> in Montreal, Canada, May 20-24, 2019.</p>    
             <!-- <p style="font-size:13px"> <strong>[2020.04]</strong> Invited talk about 3D reconstruction at Sun Yat-sen University, hosted by <a href="http://yulanguo.me/">Prof. Yulan Guo</a>.</p> -->
             <p style="font-size:13px"> <strong>[2019 - present]</strong> Reviewer: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE International Conference on Computer Vision (ICCV), European Conference on Computer Vision (ECCV), IEEE Robotics and Automation Letters (RAL), International Conference on Robotics and Automation (ICRA), International Conference on Intelligent Robots and Systems (IROS), European Conference on Mobile Robots (ECMR), International Journal of Computer Vision (IJCV), IEEE Transactions on Image Processing (T-IP), IEEE Transactions on Intelligent Transportation Systems (T-ITS), IEEE Transactions on Cybernetics (T-cyb), IEEE Transactions on Automation Science and Engineering (T-ASE), IEEE Transactions on Industrial Informatics (T-II), Neurocomputing, International Journal of Humanoid Robotics (IJHR), Geo-spatial Information Science (GIS), International Journal of Social Robotics (IJSR).</p>
            </td>
            </tr></tbody>
    </table>


    <!--SECTION 6 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Teaching</heading>
             <p> <strong>Nov. 2019 - Jun. 2020</strong>: &ensp;&ensp;  Direct four undergraduate graduation projects of SJTU, one of which was submitted to <a href="https://arxiv.org/abs/2011.12105">ICRA 2021</a>.  (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Jul. 2020 - Aug. 2020</strong>: &ensp;&ensp;  Direct seven junior undergraduates in summer laboratory practice.  (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Mar. 2020 - Mar. 2021</strong>: &ensp;&ensp; Directing four undergraduate innovation projects, two of which were approved by the Shanghai Innovation Training Program for College Students. Some students of them cooperated with me to submit <a href="https://arxiv.org/abs/2010.05762"> a TIP paper</a>, <a href="https://arxiv.org/abs/2012.10860">a T-IM paper</a>, <a href="https://arxiv.org/abs/2011.13784">a T-Cyb paper</a>. (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Nov. 2020 - Jun. 2021</strong>: &ensp;&ensp; Direct two undergraduate graduation projects of SJTU, one of which submitted <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_PWCLO-Net_Deep_LiDAR_Odometry_in_3D_Point_Clouds_Using_Hierarchical_CVPR_2021_paper.html"> a CVPR paper</a>.
             <p> <strong>Sep. 2020 - present</strong>: &ensp;&ensp; Direct three masters and a doctor in China University of Mining and Technology, because my advisor is an adjunct professor there. One of them submitted an T-II paper. One of them submitted an IJCAI2022 paper.  (<a href="http://global.cumt.edu.cn/">The China University of Mining and Technology, CUMT</a>).  
             <p> <strong>Mar. 2021 - Mar. 2022</strong>: &ensp;&ensp; Directing five undergraduate innovation projects. Some students of them cooperated with me to submit a RAL paper, two ICRA papers. (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Nov. 2021 - present</strong>: &ensp;&ensp;	Directing three undergraduate innovation projects. Some students of them cooperated with me to submit a T-NNLS paper. (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Nov. 2021 - Jun. 2022</strong>: &ensp;&ensp;	Direct ten undergraduate graduation projects of SJTU. (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Nov. 2021 - present</strong>: &ensp;&ensp;	Direct one more master in China University of Mining and Technology, because my advisor is an adjunct professor there. (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Mar. 2022 - present</strong>: &ensp;&ensp;	Directing four undergraduate innovation projects. (co-supervised with Ph.D. student Tianchen Deng.) (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Mar. 2022 - present</strong>: &ensp;&ensp;	Directing three PhD students of our laboratory and six masters who are about to enter the laboratories of China University of Mining and Technology. (<a href="http://global.cumt.edu.cn/">The China University of Mining and Technology, CUMT</a>).   </p>
             <p> <strong>Oct. 2022 - present</strong>: &ensp;&ensp;	Leader deep learning group with 23 graduate students, including 6 PhD students, 14 master students, 2 accepted PhD students, and 1 accepted master student, in IRMV lab in SJTU. (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
            
            
            
            </p>
            </td>
            </tr></tbody>
    </table>


    <!--SECTION 7 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>Mentoring</heading>
            <p>
            <strong>Doctor</strong>:
            <br>
               <strong>Chichao Cheng</strong> (SJTU), <strong>Tianchen Deng</strong> (SJTU), <strong>Mingxi Zhuang</strong> (SJTU), <strong>Nie Chang</strong> (SJTU)<br> 
               <strong>Bu Ran</strong> (SJTU), <strong>Ye Jun</strong> (SJTU), <strong>Yizhou Wang</strong> (CUMT)<br> 


            <p>
            <strong>Masters</strong>:
            <br>
            <strong>Huixin Zhang</strong> (SJTU), <strong>Zehang Shen</strong> (SJTU), <strong>Shuaiqi Ren</strong> (SJTU), <strong>Zhiheng Feng</strong> (SJTU)<br> 
            <strong>Jiuming Liu</strong> (SJTU), <strong>Itana Bulatovic</strong> (SJTU), <strong>JiaJie Jacques Xu</strong> (SJTU), <strong>Yangyi Xiao</strong> (SJTU)<br> 
            <strong>JingYun Fang</strong> (SJTU), <strong>Chaokang Jiang</strong> (CUMT), <strong>Huiying Deng</strong> (CUMT), <strong>Xiaolin Wang</strong> (CUMT)<br>
            <strong>Liang Song</strong> (CUMT), <strong>Lei Pan</strong> (CUMT), <strong>Qirong Liu</strong> (CUMT), <strong>Yiqing Xu</strong> (CUMT)
              <p>
              <strong>Undergrad Interns</strong>:
              <br>
              <strong>Huixin Zhang</strong> (SJTU), <strong>Muyao Chen</strong> (SJTU), <strong>Zehang Shen</strong> (SJTU), <strong>Yunzhe Hu</strong> (SJTU)<br>
              <strong>Jiquan Zhong</strong> (SJTU), <strong>Yanfeng Guo</strong> (SJTU), <strong>Shuyang Jiang</strong> (SJTU), <strong> Zike Cheng</strong> (SJTU)<br>
              <strong>Honghao Zeng</strong> (SJTU), <strong>Ziliang Wang</strong> (SJTU), <strong>Haolin Song</strong> (SJTU), <strong> Wenlong Yi</strong> (SJTU)<br>
              <strong>Chensheng Peng</strong> (SJTU), <strong>Shuaiqi Ren</strong> (SJTU), <strong>Jianwan Luo</strong> (SJTU), <strong> Hanmo Zheng</strong> (SJTU)<br>
              <strong>Wenhua Wu</strong> (SJTU), <strong>Xingyao Han</strong> (SJTU), <strong>Qingyu Wang</strong> (SJTU), <strong> Yuepeng Zhang</strong> (SJTU)<br>
              <strong>Tianyi Wang</strong> (SJTU), <strong>Yu Zheng</strong> (SJTU), <strong>Zhiheng Feng</strong> (SJTU), <strong> Haozhong Zou</strong> (SJTU)<br>
              <strong>Shengqi Liu</strong> (SJTU), <strong>Yufei Luo</strong> (SJTU), <strong>Lianting Huang</strong> (SJTU), <strong> jiuming Liu</strong> (HIT)<br>
              <strong>Jiahao Qiu</strong> (SJTU), <strong>Yizhe Liu</strong> (SJTU), <strong> Shijie Zhao</strong> (SJTU)
            </p>
              
            <p> 
                <strong>Past Masters</strong>:
                <br>
                <strong>Yueling Shen</strong>: &ensp;&ensp; Shanghai Jiao Tong University, SJTU, gone to PlusAI, Inc. to work.<br>
            </p>   


              <p> 
              <strong>Past Undergrad Interns</strong>:
              <br>
              
              
              <strong>Minjian Xin</strong>: &ensp;&ensp; SJTU, gone to University of California at San Diego, UCSD to pursue master. <br>             
              <strong>ShuangJie Xu</strong>: &ensp;&ensp; SJTU, gone to University of Sydney to pursue master. <br>
              <strong>Jianlong Ye</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
               <strong>Jianwei Cai</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
               <strong>Xinrui Wu</strong>: &ensp;&ensp; Shanghai Jiao Tong University, SJTU, gone to our lab to pursue master.<br>
               <strong>Yehui Yang</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
               <strong>Ruiqi Ding</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
               <strong>Hanwen Liu</strong>: &ensp;&ensp; SJTU, gone to Technische Universität München, TUM to pursue master. <br>
               <strong>Xiaoyu Tian</strong>: &ensp;&ensp; SJTU, gone to Columbia University in the City of New York to pursue master. <br>
               <strong>Chi Zhang</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>             
               <strong>Huixin Zhang</strong>: &ensp;&ensp; SJTU, gone to our lab to pursue master. <br>
               <strong>Zehang Shen</strong>: &ensp;&ensp; SJTU, gone to our lab to pursue master. <br>
               <strong>Zhiheng Feng</strong>: &ensp;&ensp; SJTU, gone to our lab to pursue master. <br>
               <strong>Shuaiqi Ren</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
               <strong>Zike Cheng</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
               <strong>Jiquan Zhong</strong>: &ensp;&ensp; SJTU, gone to Xiamen University, XMU to pursue master. <br>
               <strong>Yanfeng Guo</strong>: &ensp;&ensp; SJTU, gone to University of California, Los Angeles, UCLA to pursue master. <br>
               <strong>Tianyi Wang</strong>: &ensp;&ensp; SJTU, gone to The Hong Kong University of Science and Technology, HKUST to pursue master. <br>
               <strong>Yunzhe Hu</strong>: &ensp;&ensp; SJTU, gone to The University of Hong Kong, HKU to pursue doctor. <br>
               <strong>Ziliang Wang</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue doctor. <br>
               <strong>Honghao Zeng</strong>: &ensp;&ensp; SJTU, gone to Shanghai Baosight Software Co., Ltd to work. <br>
               <strong>Muyao Chen</strong>: &ensp;&ensp; SJTU, gone to ByteDance to work. <br>
               <strong>Haolin Song</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
               <strong>Jiuming Liu</strong>: &ensp;&ensp; HIT, gone to our lab to pursue master. <br>
               
          
          
          
          
          
          
          
          
          
            </td>
       </tr></tbody>
    </table>


    <!--SECTION 8 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>About Me</heading>
           <p align="justify">In my free time, I like reading books on psychology and literature. I like traveling.
               I also enjoy sports and meditate.
		   <!--</br></br>-->
		   <!--<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> -->
		   </p>
		   </td></tr>
       </tbody>
    </table>


    <!--SECTION 9 -->
    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="100%" align="middle">
    <p align="center" style="width: 25% ">
    <script type="text/javascript" id="clstr_globe"  src="//cdn.clustrmaps.com/globe.js?d=1yPWWuOlXo22MrO9sRinBO9GUjLHe88Yk0lOK35nmQA"></script>
    </p></td>
    </tr>
    </tbody>
    </table> -->


    <!--SECTION 10 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td><br>
              <!--<p align="right"><font size="3">Erd&ouml;s = ? </font><br> -->
              <p align="right"><font size="2"> Last update: 2022.07. Thanks for <a href="http://www.cs.berkeley.edu/~barron/">Jon Barron's</a> and <a href="https://yang7879.github.io/">Bo Yang's</a> websites.</font></p>
           </td>
        </tr>
        </tbody>
    </table>


</td>
</tr>
</tbody>
</table>
</body>
</html>
