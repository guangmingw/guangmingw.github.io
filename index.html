<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>Guangming Wang | The Shanghai Jiao Tong University</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/jpg" href="./imgs/sjtu_icon.png">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>


    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center"><name>Guangming Wang</name></p>
                  <p align="justify">I am a Ph.D. student (2018.09-) in the <a href="http://irmv.sjtu.edu.cn/"> Intelligent Robotics and Machine Vision (IRMV) Lab</a>
                    at <a href="http://en.sjtu.edu.cn/">the Shanghai Jiao Tong University</a> advised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=q6AY9XsAAAAJ">Hesheng Wang</a>. I obtained my B.Eng. degree (2014.09-2018.06)
                    from <a href="http://en.csu.edu.cn/">the Central South University</a>, where I was in the Physics Sublimation Honor Class.

                    <!-- </br></br>
                    In my D.Phil study, I interned at the Augumented Reality team of <a href="http://www.a9.com/">Amazon</a> (Palo Alto, CA).
                    In my M.Phil study, I interned at <a href="https://www.astri.org/">Hong Kong Applied Science and Technology Research Institute</a>.
                    In my undergraduate study, I was an exchange student at <a href="http://www.upv.es/">Universitat Politècnica de València</a> (Valencia, Spain). -->

	            </br>
                </p><p align="center">
                    <a href="mailto:wangguangming@sjtu.edu.cn">Email</a> /
                    <a href="https://scholar.google.com/citations?hl=en&user=GGHfHSIAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/guangmingw"> Github </a>
                </p>
              </td>
			  <td align="right"> <img class="hp-photo" src="./imgs/photo.jpg" style="width: 240;"></td></tr>
            </tbody>
          </table>

    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Research</heading>
              <p align="justify">My research interests include computer vision for autonomous robots and robot learning, specifically on topics such as visual/LiDAR odometry, depth estimation, optical/scene flow estimation, semantic segmentation, 3D point cloud learning, and deep reinforcement learning for robots.
		   </p></br>
		   <!-- <font color="red"><strong>Openings:</strong> </font></br></br>

            <font color="red">Several fully funded PhD positions and Research Assistantships are available now. Welcome to drop me an email with your CV and transcripts. </font> -->
		   </td></tr>
       </tbody>
    </table>

    <!--SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>News</heading>
            <!-- <p style="font-size:13px"> <strong>[2020.11.24]</strong>  Our latest paper SpinNet is on <a href="https://arxiv.org/abs/2011.12149">arXiv</a>.</p>

            <p style="font-size:13px"> <strong>[2020.10.12]</strong>  Our paper GRF is on <a href="http://arxiv.org/abs/2010.04595">arXiv</a>.</p>

            <p style="font-size:13px"> <strong>[2020.09.10]</strong> Pass my D.Phil thesis,
                examined by Profs. <a href="https://mpawankumar.info/">M. Pawan Kumar </a> and <a href="https://www.doc.ic.ac.uk/~ajd/">Andrew Davison</a>.</p>

            <p style="font-size:13px"> <strong>[2020.03.08]</strong> Invited to present our <a href="https://arxiv.org/abs/1911.11236">RandLA-Net</a>
                and <a href="https://arxiv.org/abs/1906.01140">3D-BoNet</a> at Shenlan.
                Here are the <a href="https://www.shenlanxueyuan.com/open/course/53">Video</a>
                and <a href="https://www.dropbox.com/s/80w91bqzfdl5vow/%E6%B7%B1%E8%93%9D%E5%AD%A6%E9%99%A2%E5%85%AC%E5%BC%80%E8%AF%BE_20200308.pdf?dl=0">Slides</a>. </p>
            <p style="font-size:13px"> <strong>[2020.02.27]</strong> One co-authored <a href="https://arxiv.org/abs/1911.11236">paper</a> for 3D semantic segmentation is accepted by <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>. </p>
            <p style="font-size:13px"> <strong>[2019.10.24]</strong> Pass my
                <a href="https://www.ox.ac.uk/students/academic/guidance/graduate/research/status/DPhil?wssl=1">D.Phil confirmation</a>, examined by Profs.
                <a href="https://scholar.google.co.uk/citations?user=UZ5wscMAAAAJ&hl=en">Andrew Zisserman</a> and
                <a href="https://www.cs.ox.ac.uk/people/alessandro.abate/home.html">Alessandro Abate</a>.</p>
            <p style="font-size:13px"> <strong>[2019.09.03]</strong> One first-authored paper is accepted as a spotlight at <a href="https://nips.cc/Conferences/2019">NeurIPS 2019</a>. </p> -->
            <p style="font-size:13px"> <strong>[2021.05.18]</strong> One co-first-author <a href="https://ieeexplore.ieee.org/abstract/document/9435105/">paper</a> for 3D scene flow is accepted by top journal TIP 2021</a>! </p>
            <p style="font-size:13px"> <strong>[2021.03.01]</strong> One co-first-author <a href="https://arxiv.org/abs/2012.00972">paper</a> for 3D LiDAR odometry is accepted by <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>! </p>
            <p style="font-size:13px"> <strong>[2021.03.01]</strong> One co-first-author paper for 3D scene flow is accepted by <a href="http://www.icra2021.org/">ICRA 2021</a>! </p>
            <p style="font-size:13px"> <strong>[2020.11.16]</strong> One co-first-author paper is submitted to CVPR 2021! </p>
            <p style="font-size:13px"> <strong>[2020.10.31]</strong> One co-first-author paper and one second-author paper are submitted to ICRA 2021! </p>
          </td>
       </tr></tbody>
    </table>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading>Publications / Preprints</heading>
          </td>
          </tr></tbody>
    </table>

    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr>
        <td width="20%"><img src="./imgs/20_arxiv_3d_odometry.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/abs/2012.00972">
                 <papertitle>PWCLO-Net: Deep LiDAR Odometry in 3D Point Clouds Using Hierarchical Embedding Mask Optimization</papertitle></a>
                 <br> <strong>G. Wang*</strong>, X. Wu*, Z. Liu, and H. Wang<br>
                 <a href="https://arxiv.org/abs/2012.00972"> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021</a>
                 <!-- <a href="https://arxiv.org/abs/2012.00972">arXiv</a> / -->
                 <!-- <a href="https://github.com/QingyongHu/SpinNet"><font color="red">Code</font></a> -->
                 <!-- <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SpinNet&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe> -->
                 <br>(* indicates equal contributions)
                 <p align="justify" style="font-size:13px">We introduce a novel 3D point cloud learning model for deep LiDAR odometry, named PWCLO-Net, using hierarchical embedding mask optimization. It outperforms all recent learning-based methods and outperforms the geometry-based approach, LOAM with mapping optimization, on most sequences of the KITTI odometry dataset.
                 </p>
                <p></p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/20_arxiv_un_scene_flow.png" alt="PontTuset" width="180" style="border-style: none"></td>
        <td width="80%" valign="top">
            <p><a href="">
            <papertitle>Unsupervised Learning of Scene Flow from Monocular Camera</papertitle></a>
            <br><strong>G. Wang*</strong>, X. Tian*, R. Ding, and H. Wang<br>
            
            International Conference on Robotics and Automation (ICRA), Xi'an, China, 2021. 
            <!-- <a href="https://arxiv.org/abs/2003.02392">arXiv, 2020</a> -->
            <br>(* indicates equal contributions)

            </p><p></p>
            <p align="justify" style="font-size:13px">We present a framework to realize the unsupervised learning of scene flow from monocular camera. </p>
        </td>
    </tr>

        <td width="20%"><img src="./imgs/20_arxiv_ASTAConv.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/abs/2012.10860">
                 <papertitle>Anchor-Based Spatial-Temporal Attention Convolutional Networks for Dynamic 3D Point Cloud Sequences</papertitle></a>
                 <br><strong>G. Wang</strong>, H. Liu, M. Chen, Y. Yang, Z. Liu, and H. Wang<br>
                 <a href="https://arxiv.org/abs/2012.10860"> arXiv, 2020</a>
                 <br>
                  <!--<font color="red"><strong>..</strong></font><br>-->
                  Submitted to  IEEE Transactions on Multimedia (T-MM), under review.
                 <!-- <a href="http://arxiv.org/abs/2009.03137">arXiv</a> /
                 <a href="https://www.youtube.com/watch?v=IG0tTdqB3L8">Demo</a> /
                 <a href="https://github.com/QingyongHu/SensatUrban"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SensatUrban&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe> -->
                <!-- <br>(* indicates corresponding author) -->
                <!-- <br>(* indicates equal contributions) -->
                <p align="justify" style="font-size:13px">We introduce an Anchor-based Spatial-Temporal Attention Convolution operation (ASTAConv) to process dynamic 3D point cloud sequences.  It makes better use of the structured information within the local region, and learn spatial-temporal embedding features from dynamic 3D point cloud sequences.
                  </p>
                <p></p>
            </td>
        </tr>




        <td width="20%"><img src="./imgs/arxiv_Spherical_ Interpolated.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/abs/2011.13784">
                 <papertitle>Spherical Interpolated Convolutional Network with Distance-Feature Density for 3D Semantic Segmentation of Point Clouds</papertitle></a>
                 <br><strong>G. Wang*</strong>, Y. Yang*, Z. Liu, and H. Wang <br>
                 <a href="https://arxiv.org/abs/2011.13784"> arXiv, 2020</a>
                 <br>
                 Submitted to IEEE Transactions on Cybernetics (T-Cyb), under review.
                 <!-- / -->
                 <!-- <a href="https://github.com/alextrevithick/GRF"><font color="red">Code</font></a> -->
                 <!-- <iframe src="https://ghbtns.com/github-btn.html?user=alextrevithick&repo=GRF&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe> -->
                 <br>(* indicates equal contributions)
                 <p align="justify" style="font-size:13px">We introduce  a spherical interpolated convolution operator to replace the traditional grid-shaped 3D convolution operator. It improves the accuracy and reduces the parameters of the network.
                 </p>
                <p></p>
            </td>
        </tr>


                <td width="20%"><img src="./imgs/20_arxiv_HAFlowNet.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/abs/2010.05762">
                 <papertitle>Hierarchical Attention Learning of Scene Flow in 3D Point Clouds</papertitle></a>
                 <br><strong>G. Wang*</strong>, X. Wu*, Z. Liu, and H. Wang<br>
		<em>IEEE Transactions on Image Processing (TIP)</em>, 2021 <font color="red"><strong>(IF=9.340)</strong></font><br>
		        <a href="https://arxiv.org/abs/2010.05762">arXiv</a>/
		        <a href="https://ieeexplore.ieee.org/abstract/document/9435105/">IEEE Xplore</a>
                 <br>
                  <!--<font color="red"><strong>..</strong></font><br>-->
                (* indicates equal contributions)
                <p align="justify" style="font-size:13px">We introduce a novel hierarchical neural network with double attention for learning the correlation of point features in adjacent frames and refining scene flow from coarse to fine layer by layer. It has a new more-for-less hierarchical architecture. The proposed network achieves the state-of-the-art performance of 3D scene flow estimation on the FlyingThings3D and KITTI Scene Flow 2015 datasets.
                  </p>
                <p></p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/20_arxiv_DDPGwB.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2011.12105">
	            <papertitle>Achieving Sample-Efficient and Online-Training-Safe Deep Reinforcement Learning with Base Controllers</papertitle></a>
                <br>M. Xin, <strong>G. Wang</strong>, Z. Liu, and H. Wang
                <br>
                <a href="https://arxiv.org/abs/2011.12105"> arXiv, 2020</a>
                <br>
                <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
                </p><p></p>
			    <p align="justify" style="font-size:13px">We introduce a method of learning challenging sparse-reward tasks utilizing existing controllers. Compared to previous works of learning from demonstrations, our method improves sample efficiency by orders of magnitude and can learn online in a safe manner.  </p>
            </td>
        </tr>

	    <td width="20%"><img src="./imgs/DOPlearning.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                <p><a href="https://ieeexplore.ieee.org/document/9152137">
	            <papertitle>Unsupervised Learning of Depth, Optical Flow and Pose with Occlusion From 3D Geometry</papertitle></a>
                <br><strong>G. Wang</strong>, C. Zhang, H. Wang, J. Wang, Y. Wang, and X. Wang<br>
                <em>IEEE Transactions on Intelligent Transportation Systems (T-ITS)</em>, 2020 <font color="red"><strong>(IF=6.319)</strong></font><br>
		        <a href="https://arxiv.org/abs/2003.00766">arXiv</a>/
		        <a href="https://ieeexplore.ieee.org/document/9152137">IEEE Xplore</a>/
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/NXr2-AOrl_VllIs_2buLwA"><font color="red">(DeepBlue深兰科技,</font></a>
                <a href="https://ifosa.org/invited.html"><font color="red">The First International Forum on 3D Optical Sensing and Applications (iFOSA 2020),</font></a>
                <!-- <a href="https://mp.weixin.qq.com/s/C1FDPkAkmnmAZ_gvvtzBHw"><font color="red">AI科技评论,</font></a>
                <a href="https://mp.weixin.qq.com/s/wViZITtsb4j3oFtOpJI9wQ"><font color="red">将门创投,</font></a>
                <a href="https://mp.weixin.qq.com/s/S7mHrOxOwTIhDGPhu1SI4A"><font color="red">CVer,</font></a> -->
                <a href="https://mp.weixin.qq.com/s/GaS2F-2-aEQIJ-6ZLahynQ"><font color="red">计算机视觉life)</font>/</a>
                <a href="https://www.youtube.com/watch?v=Y8Up9OsZbcg"><font color="red">Video</font></a>/
                <a href="https://github.com/Yang7879/AttSets"><font color="red">Code</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=guangmingw&repo=DOPlearning&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose a method to explicitly handle occlusion, propose the less-than-mean mask, the maximum normalization, and the consistency of depth-pose and optical flow in the occlusion regions. </p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/occlusion_mask.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://ieeexplore.ieee.org/abstract/document/8793622">
	             <papertitle>Unsupervised Learning of Monocular Depth and Ego-Motion Using Multiple Masks</papertitle></a>
                 <br><strong>G. Wang</strong>, H. Wang, Y. Liu, and W. Chen
                 <br>
                 <em>International Conference on Robotics and Automation (ICRA)</em>, Montreal, Canada, 2019 
                 <br>
                 <!--<font color="red"><strong>..</strong></font><br>-->
                 <a href="https://ieeexplore.ieee.org/abstract/document/8793622">IEEE Xplore</a> /
                 <!-- <a href="http://kaldir.vc.in.tum.de/scannet_benchmark/result_details?id=118">ScanNet Benchmark</a> / -->
                 <!-- <a href="https://www.reddit.com/r/MachineLearning/comments/bx8jhz/r_new_sota_for_3d_object_detection/">Reddit Discussion</a> / -->
                 <font color="red"> News:</font>
                 <a href="https://mp.weixin.qq.com/s/sKSb-dLRTifiwK9hcx_Vcg"><font color="red">(泡泡机器人,</font></a>
                 <!-- <a href="https://mp.weixin.qq.com/s/4GPkmTri4Vk7Xy0J8TiBNw"><font color="red">图像算法,</font></a>
                 <a href="https://mp.weixin.qq.com/s/C1FDPkAkmnmAZ_gvvtzBHw"><font color="red">AI科技评论,</font></a>
                 <a href="https://mp.weixin.qq.com/s/wViZITtsb4j3oFtOpJI9wQ"><font color="red">将门创投,</font></a>
                 <a href="https://mp.weixin.qq.com/s/S7mHrOxOwTIhDGPhu1SI4A"><font color="red">CVer,</font></a> -->
                 <a href="https://mp.weixin.qq.com/s/-IcQKYpXcs6CPme8d7Jbww"><font color="red">上海交大研究生教育)</font>/</a>
                 <a href="https://ieeexplore.ieee.org/ielx7/8780387/8793254/8793622/2533_MM.zip?tp=&arnumber=8793622"><font color="red">Video</font></a>/
                 <a href="https://github.com/guangmingw/DOPlearning"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=guangmingw&repo=DOPlearning&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
			     <p align="justify" style="font-size:13px">We propose a new unsupervised learning method of depth and ego-motion using multiple masks to handle the occlusion problem.</p>
                <p></p>
            </td>
        </tr>



	   
        </tbody>
    </table>


    <!--SECTION 6 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Talks & Academic Services</heading>
             <p style="font-size:13px"> <strong>[2019.05]</strong> Conference Talk on “Unsupervised Learning of Monocular Depth and Ego-Motion Using Multiple Masks” at <a href="https://mp.weixin.qq.com/s/-IcQKYpXcs6CPme8d7Jbww">ICRA 2019</a> in Montreal, Canada, May 20-24, 2019.</p>
             <p style="font-size:13px"> <strong>[2019.07]</strong> Seminar Talk on “Unsupervised Learning of Monocular Depth and Ego-Motion Using Multiple Masks” at <a href="./imgs/SEEEP.mp4">Sino-European Engineering Education Platform (SEEEP) Doctoral Summer School</a> in Instituto Superior Técnico (IST), Lisbon, Portugal, July 22-25, 2019.</p>
             <p style="font-size:13px"> <strong>[2020.10]</strong> Invited Talk on “Unsupervised Learning of Depth, Optical Flow and Pose with Occlusion From 3D Geometry” at <a href="https://ifosa.org/invited.html"> the First International Forum on 3D Optical Sensing and Applications (iFOSA 2020)</a> held in Being, China, October 17-18, 2020.</p>
             <!-- <p style="font-size:13px"> <strong>[2020.04]</strong> Invited talk about 3D reconstruction at Sun Yat-sen University, hosted by <a href="http://yulanguo.me/">Prof. Yulan Guo</a>.</p> -->
             <p style="font-size:13px"> <strong>[2019 -]</strong> Reviewer: IEEE Robotics and Automation Letters (RAL), International Conference on Robotics and Automation (ICRA), International Conference on Intelligent Robots and Systems (IROS).</p>
            </td>
            </tr></tbody>
    </table>


    <!--SECTION 6 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Teaching</heading>
             <p> <strong>Nov. 2019 - Jun. 2020</strong>: &ensp;&ensp;  Direct four undergraduate graduation projects of SJTU, one of which was submitted to <a href="https://arxiv.org/abs/2011.12105">ICRA 2021</a>.  (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Jul. 2020 - Aug. 2020</strong>: &ensp;&ensp;  Direct seven junior undergraduates in summer laboratory practice.  (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Mar. 2020 - present</strong>: &ensp;&ensp; Directing four undergraduate innovation projects, two of which were approved by the Shanghai Innovation Training Program for College Students. Some students of them cooperated with me to submit <a href="https://arxiv.org/abs/2010.05762"> a TIP paper</a>, a CVPR paper, <a href="https://arxiv.org/abs/2012.10860">a T-MM paper</a>, <a href="https://arxiv.org/abs/2011.13784">a T-Cyb paper</a>. (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Sep. 2020 - present</strong>: &ensp;&ensp; Direct three masters and a doctor in China University of Mining and Technology, because my advisor is an adjunct professor there.  (<a href="http://global.cumt.edu.cn/">The China University of Mining and Technology, CUMT</a>). </p>
            </td>
            </tr></tbody>
    </table>


    <!--SECTION 7 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>Mentoring</heading>
            <p>
            <strong>Doctor</strong>:
            <br>
               <strong>Yizhou Wang</strong> (Dec 2020 - ): &ensp;&ensp; China University of Mining and Technology, CUMT.</p>
            

            <p>
            <strong>Masters</strong>:
            <br>
              <strong>Chaokang Jiang</strong> (Sep 2020 - ): &ensp;&ensp; CUMT.<br>
              <strong>Huiying Deng (CUMT)</strong> (Sep 2020 - ): &ensp;&ensp; CUMT.</p>



              <p>
              <strong>Undergrad Interns</strong>:
              <br>
              <strong>Xinrui Wu</strong> (Shanghai Jiao Tong University, SJTU, admitted to SJTU to pursue master)<br>
              <strong>Xiaoyu Tian</strong> (SJTU), <strong>Huixin Zhang</strong> (SJTU), <strong>Muyao Chen</strong> (SJTU), <strong>Zehang Shen</strong> (SJTU)<br>
              <strong>Jiquan Zhong</strong> (SJTU), <strong>Yanfeng Guo</strong> (SJTU), <strong>Shuyang Jiang</strong> (SJTU), <strong>Jianwan Luo</strong> (SJTU)<br>
              <strong>Chensheng Peng</strong> (SJTU), <strong>Yunzhe Hu</strong> (SJTU), <strong>Yongxiang Liu</strong> (SJTU), <strong>Shuaiqi Ren</strong> (SJTU)<br>
              <strong>Honghao Zeng</strong> (SJTU), <strong>Ziliang Wang</strong> (SJTU), <strong>Haolin Song</strong> (SJTU)<br>
              <strong>Wenhua Wu</strong> (SJTU), <strong>Xingyao Han</strong> (SJTU), <strong>Yunlang Zhou</strong> (SJTU)</p>
              
              
              <p> 
              <strong>Past Undergrad Interns</strong>:
              <br>
               <strong>Minjian Xin</strong> (Oct 2019 - Feb 2021): &ensp;&ensp; SJTU, gone to University of California at San Diego, UCSD to pursue master. <br>             
               <strong>Jianlong Ye</strong> (Oct 2019 - Aug 2020): &ensp;&ensp; SJTU, gone to SJTU as a research assistant. <br>
               <strong>ShuangJie Xu</strong> (Oct 2019 - Jul 2020): &ensp;&ensp; SJTU, gone to SJTU as a research assistant. <br>
               <strong>Jianwei Cai</strong> (Jun 2019 - Aug 2020): &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
               <strong>Ruiqi Ding</strong> (Nov 2019 - Feb 2021): &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
               <strong>Yehui Yang</strong> (Apr 2019 - Dec 2020): &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
               <strong>Chi Zhang</strong> (Apr 2019 - Sep 2020): &ensp;&ensp; SJTU, gone to SJTU to pursue master. </p>
          </td>
       </tr></tbody>
    </table>


    <!--SECTION 8 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>About Me</heading>
           <p align="justify">In my free time, I like reading books on psychology and literature. I like traveling.
               I also enjoy sports and meditate.
		   <!--</br></br>-->
		   <!--<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> -->
		   </p>
		   </td></tr>
       </tbody>
    </table>


    <!--SECTION 9 -->
    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="100%" align="middle">
    <p align="center" style="width: 25% ">
    <script type="text/javascript" id="clstr_globe"  src="//cdn.clustrmaps.com/globe.js?d=1yPWWuOlXo22MrO9sRinBO9GUjLHe88Yk0lOK35nmQA"></script>
    </p></td>
    </tr>
    </tbody>
    </table> -->


    <!--SECTION 10 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td><br>
              <!--<p align="right"><font size="3">Erd&ouml;s = ? </font><br> -->
              <p align="right"><font size="2"> Last update: 2021.05. Thanks for <a href="http://www.cs.berkeley.edu/~barron/">Jon Barron's</a> and <a href="https://yang7879.github.io/">Bo Yang's</a> websites.</font></p>
           </td>
        </tr>
        </tbody>
    </table>


</td>
</tr>
</tbody>
</table>
</body>
</html>
