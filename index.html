<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>Guangming Wang | The University of Cambridge</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/jpg" href="./imgs/UOC_icon.jpg">
</head>

<body>
    <script src="script/functions.js"></script>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>


    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center"><name>Guangming Wang</name></p>
                  <p align="justify">I am a Postdoctoral Research Associate of the <a href="https://www.eng.cam.ac.uk/"> Department of Engineering</a> at the <a href="https://www.cam.ac.uk/">University of Cambridge</a> advised by Prof. <a href="https://www.construction.cam.ac.uk/staff/dr-brian-sheil">Brian Sheil</a>.
                    
                    <p align="justify">I obtain my Ph.D. degree in the <a href="http://irmv.sjtu.edu.cn/"> Intelligent Robotics and Machine Vision (IRMV) Lab</a>
                    at <a href="http://en.sjtu.edu.cn/">the Shanghai Jiao Tong University</a> advised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=q6AY9XsAAAAJ">Hesheng Wang</a>.

                    <p align="justify">I have a lot of collaborations with Prof. <a href="https://msc.berkeley.edu/people/tomizuka.html"> Masayoshi Tomizuka</a> from <a href="https://www.berkeley.edu/">UC Berkeley</a>.
                    I spent a happy time as a visiting researcher in the <a href="http://www.cvg.ethz.ch/index.php"> Computer Vision and Geometry Group (CVG)</a>, <a href="https://ethz.ch/en.html">ETH Zurich</a>, advised by <a href="https://people.inf.ethz.ch/pomarc/"> Prof. Marc Pollefeys</a>.
                    
                    <p align="justify">I am an Associate Editor (AE) for RAL, ICRA2024, 2025, 2026, and IROS2024, 2025. I was awarded <a href="https://www.daad.de/en/the-daad/postdocnet/">the DAAD AI and Robotics fellow</a>. I obtained twice Scholarships for Doctoral
                        Students (Top 1% in Shanghai Jiao Tong University). 
                        My doctoral thesis received <a href="https://www.gs.sjtu.edu.cn/yxbslw">the Excellent Doctoral Dissertation Award from Shanghai Jiao Tong University</a> (15 recipients university-wide annually, the only one in Department of Automation)!
                        
                        <p align="justify">Most of my co-mentored undergraduates have gone to UC Berkeley, Princeton, HKU, Columbia, Gatech, UCSD, UCLA, TUM, SJTU and so on. Some of them got full scholarships for direct PhD students. <font color="red">Welcome to contact me to come to the University of Cambridge for a visiting student or remote collaboration!</font></a>
                    
                    
                        <!-- </br></br>
                    In my D.Phil study, I interned at the Augmented Reality team of <a href="http://www.a9.com/">Amazon</a> (Palo Alto, CA).
                    In my M. Phil study, I interned at <a href="https://www.astri.org/">Hong Kong Applied Science and Technology Research Institute</a>.
                    In my undergraduate study, I was an exchange student at <a href="http://www.upv.es/">Universitat Politècnica de València</a> (Valencia, Spain). -->

	            </br>
                </p><p align="center">
                    <a href="mailto:gw462@cam.ac.uk">Email</a> /
                    <a href="https://scholar.google.com/citations?hl=en&user=GGHfHSIAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/guangmingw"> Github </a>
                </p>
              </td>
			  <td align="right"> <img class="hp-photo" src="./imgs/Guangming2.jpg" style="width: 240;"></td></tr>
            </tbody>
          </table>

    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Research</heading>
          <p align="justify">My area of focus is on developing robust robot perception, localization, mapping methods,
             enabling intelligent understanding of the real world while ensuring safety and efficiency. 
             These keywords, intelligence, safety, and efficiency, make the research technology widely and practically
              applicable in areas such as autonomous mobile robots, robot manipulation, 
              and digital twin construction. My long-term vision is to achieve
               full understanding and utilization of real-world scenes by robots in terms
                of geometry, semantics, concepts, and logic, enabling AI robots to closely
                 collaborate with humans.</p>


              <p align="justify">My research interests include robot perception, localization, mapping, planning, and construction automation based on deep learning/reinforcement learning, specifically on topics such as:</p>
              <ul>
                <li><p align="justify">Robot perception: depth estimation, optical/scene flow estimation, semantic segmentation, object detection/tracking, 3D point cloud learning.</p></li>
                <li><p align="justify">Robot localization: visual/LiDAR odometry, 2D/3D registration, robot relocalization.</p></li>
                <li><p align="justify">Robot mapping: dynamic mapping, implicit neural field-based mapping.</p></li>
                <li><p align="justify">Robot planning: reinforcement learning for manipulator tasks.</p></li>
                <li><p align="justify">Construction Automation: 3D reconstrction of buildings/roads, semantic segmentation and BIM generation of buildings, BIM model retrieval, digital twin, robot construction.</p></li>
              </ul>

		   </br>

           
		   <!-- <font color="red"><strong>Openings:</strong> </font></br></br>

            <font color="red">Several fully funded PhD positions and Research Assistantships are available now. Welcome to drop me an email with your CV and transcripts. </font> -->
		   </td></tr>
       </tbody>
    </table>

    <!--SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>News</heading>

            <p style="font-size:13px"> <strong>[2025.09.11]</strong> I am invited to give a presentation related to Real2Sim2Real on <a href="https://cambridgecv-workshop-2025sep.limitlab.xyz/"> Cambridge Computer Vision Workshop</a>! <a href="files/Cambridge Computer Vision Workshop.pdf" target="_blank">The slides</a>.</p> 
  
<p style="font-size:13px"> <strong>[2025.09.11]</strong> I will serve as Associate Editor (AE) for the top robotics conference <a href="https://2026.ieee-icra.org/"> ICRA 2026</a>! </p> 
             
            <p style="font-size:13px"> <strong>[2025.07.02]</strong> One first-author <a href="https://arxiv.org/abs/2405.01333">survey paper</a> on NeRF in robotics is accepted by top journal IJRR 2025<font color="red"> (IF=5.200)</font>! </p>
            
            <p style="font-size:13px"> <strong>[2025.06.28]</strong> One first-author <a href="https://ieeexplore.ieee.org/abstract/document/11078010/">paper</a> on 2D-3D Registration is published by top journal T-TRO 2025<font color="red"> (IF=10.500)</font>! </p>
            
            <p style="font-size:13px"> <strong>[2025.02.26]</strong> One first-author <a href="https://ieeexplore.ieee.org/abstract/document/10906337/">paper</a> on LiDAR odometry and 3D scene flow is published by top journal T-ITS 2025<font color="red"> (IF=9.551)</font>! </p>
            <p style="font-size:13px"> <strong>[2025.02.13]</strong> One corresponding-author <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10884537">paper</a> on General Gaussian Mapping for various modalities is published by top journal T-ASE 2025<font color="red"> (IF=5.999)</font>! </p>
            
            <p style="font-size:13px"> <strong>[2025.01.28]</strong> I wil serve as Associate Editor (AE) for the top robotics conference <a href="http://www.iros25.org/"> IROS 2025</a>! </p> 
            <p style="font-size:13px"> <strong>[2024.01.27]</strong> <a href="https://arxiv.org/abs/2403.11776">One paper</a> <font color="red">(PhD student ranks first co-directed by me)</font> on dynamic Neural SLAM and <a href="https://arxiv.org/abs/2409.20291">one paper</a> <font color="red">(undergraduate student ranks first co-directed by me)</font> on Real2Sim2Real Robotic Manipulation Learning are accepted by top Robotics conference <a href="https://2025.ieee-icra.org/">ICRA 2025</a>! </p> 
        
            <p style="font-size:13px"> <strong>[2024.09.25]</strong> One co-first-author <font color="red">(PhD student ranks first co-directed by me)</font> <a href="https://arxiv.org/abs/2311.17491">paper</a> on LiDAR Point Cloud Semantic Segmentation is accepted by top AI conference <a href="https://nips.cc/">NIPS 2024</a>! </p> 
            
            <p style="font-size:13px"> <strong>[2024.09.16]</strong> I wil serve as Associate Editor (AE) for the top robotics conference <a href="https://2025.ieee-icra.org/"> ICRA 2025</a>! </p> 
                
            <p style="font-size:13px"> <strong>[2024.07.24]</strong> I start serving on the <a href="https://www.ieee-ras.org/publications/ra-l"> IEEE Robotics and Automation Letters (RA-L)</a> Editorial Board as Associate Editor in the area Visual Perception and Learning! </p> 

            <!-- <p style="font-size:13px"> <strong>[2024.07.07]</strong> Congratulations on the success of the workshop <a href="https://luke502320386.github.io/index_en.html">“3D Reconstruction Technology and Applications”</a> at University of Cambridge!</p>  -->
                
            <p style="font-size:13px"> <strong>[2024.07.01]</strong> One third-author <font color="red">(PhD student and master student rank first and second co-directed by me)</font> paper on large-scale road surface reconstruction based
                on explicit mesh and implicit encoding is accepted by top computer vision conference <a href="https://eccv2024.ecva.net/Conferences/2024">ECCV 2024</a>! </p> 
             
            <p style="font-size:13px"> <strong>[2024.06.30]</strong> One co-first-author <font color="red">(master student ranks first co-directed by me)</font> paper on Deep Sequence LiDAR Odometry is accepted by top Robotics conference <a href="https://iros2024-abudhabi.org/">IROS 2024</a>! </p> 
            
            <p style="font-size:13px"> <strong>[2024.06.14]</strong> I organize a workshop <a href="https://luke502320386.github.io/index_en.html">“3D Reconstruction Technology and Applications”</a> with colleagues at University of Cambridge on July 7, 2024. Welcome to registrate and participate!</p> 
            <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
                <div id="old_news" style="display: none;">
            <p style="font-size:13px"> <strong>[2024.06.17]</strong> One penultimate-author <font color="red">(PhD student ranks first co-directed by me)</font> <a href="https://link.springer.com/chapter/10.1007/978-3-031-72089-5_17">paper</a> on the  4D
                Deformable Reconstruction is accepted by top medical image conference <a href="https://miccai.org/">MICCAI 2024</a>! </p>
             
            <p style="font-size:13px"> <strong>[2024.03.18]</strong> My doctoral dissertation "Image-Point Cloud Soft Correspondence based Robot Multi-Modal Localization in Dynamic and Complex Environment" received <a href="https://www.gs.sjtu.edu.cn/yxbslw">the Excellent Doctoral Dissertation Award from Shanghai Jiao Tong University</a> <font color="red">(15 recipients annually, the only one in Department of Automation)</font>! </p> / News: (<a href="https://mp.weixin.qq.com/s/xdzKnBLzQAPk8AYiQ5pyig"><font color="red">Department of Automation</font></a>) </p>
                
            <p style="font-size:13px"> <strong>[2024.02.27]</strong> Three co-first-author or second-author <font color="red">(master or PhD students rank first co-directed by me)</font> papers on 3D scene flow, auto-labelling, and Nerf-SLAM are accepted by top computer vision conference <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a>! </p> 
                
            <p style="font-size:13px"> <strong>[2024.01.23]</strong> I wil serve as Associate Editor (AE) for the top robotics conference <a href="https://iros2024-abudhabi.org/"> IROS 2024</a>! </p> 

                <p style="font-size:13px"> <strong>[2023.09.11]</strong> I wil serve as Associate Editor (AE) for the top robotics conference <a href="https://2024.ieee-icra.org/"> ICRA 2024</a>! </p> 
                
                <p style="font-size:13px"> <strong>[2023.07.20]</strong> Join <a href="https://cit.eng.cam.ac.uk/"> Construction Information Technology (CIT) Laboratory</a>, <a href="https://www.cam.ac.uk/">University of Cambridge</a> as a Research Associate! </p> 
                <p style="font-size:13px"> <strong>[2023.07.14]</strong> Three co-first-author <font color="red">(undergraduate and Junior graduate rank first co-directed by me)</font> papers on 3D scene flow, point cloud registration, and robust estimation are accepted by top computer vision conference <a href="https://iccv2023.thecvf.com/">ICCV 2023</a>! </p> 
                
                <p style="font-size:13px"> <strong>[2023.01.17]</strong> One co-first-author <font color="red">(undergraduate ranks first co-directed by me)</font> paper on self-supervised learning of depth and pose with pseudo-LiDAR point cloud is accepted by top robotics conference <a href="https://www.icra2023.org/">ICRA 2023</a>! </p> 
                <p style="font-size:13px"> <strong>[2022.12.08]</strong> Got Shanghai Jiao Tong University <a href="https://www.gs.sjtu.edu.cn/info/1115/9386.htm">"Academic Star" Nomination Award</a> <font color="red"> (Top 0.1%)</font>! </p> / News: (<a href="https://mp.weixin.qq.com/s/ym7NS_7vJpBfpgxrrR4d7A"><font color="red">Shanghai Jiao Tong University</font></a>, <a href="https://mp.weixin.qq.com/s/ACEisGNbhbIipRIRK2fkXw"><font color="red">School of Electronic Information and Electrical Engineering</font></a>) </p>
               
                <p style="font-size:13px"> <strong>[2022.11.19]</strong> One co-first-author <font color="red">(undergraduate ranks first co-directed by me)</font> paper on the learning of LiDAR odometry with Transformer is accepted by top AI conference <a href="https://aaai.org/Conferences/AAAI-23/">AAAI 2023</a>! </p>
                
                <p style="font-size:13px"> <strong>[2022.10.11]</strong> One first-author <a href="https://ieeexplore.ieee.org/abstract/document/9924173">paper</a> for the unsupervised learning of depth and pose is accepted by top journal TCSVT 2022<font color="red"> (IF=5.859)</font>! </p>
                
                <p style="font-size:13px"> <strong>[2022.09.26]</strong> Got Scholarships for Doctoral Students <font color="red"> again (Top 1%)</font>! </p>
               
                <p style="font-size:13px"> <strong>[2022.09.23]</strong> One co-first-authored <font color="red">(master student ranks first co-directed by me)</font> <a href="https://ieeexplore.ieee.org/abstract/document/9905954">paper</a> for pseudo-LiDAR 3D scene flow estimation is accepted by IEEE Transactions on Industrial Informatics <font color="red">(T-II, IF=11.648)</font>！</p> 
            
                <p style="font-size:13px"> <strong>[2022.09.13]</strong> One first-author <a href="https://arxiv.org/abs/2111.02135">paper</a> for the efficient 3D deep LiDAR odometry is accepted by top journal TPAMI 2022<font color="red"> (IF=24.314)</font>! </p>
                
                <p style="font-size:13px"> <strong>[2022.08.23]</strong> One first-author <a href="https://ieeexplore.ieee.org/abstract/document/9882014">paper</a> for the robot manipulator learning is accepted by top journal TNNLS 2022<font color="red"> (IF=14.255)</font>! </p>
                <p style="font-size:13px"> <strong>[2022.07.03]</strong> One first-author <a href="https://link.springer.com/chapter/10.1007/978-3-031-19827-4_3">paper</a> for the 3D scene flow estimation is accepted by top computer vision conference <a href="https://eccv2022.ecva.net/">ECCV 2022</a>! </p>
                
                <p style="font-size:13px"> <strong>[2022.06.23]</strong> Join <a href="http://www.cvg.ethz.ch/index.php"> Computer Vision and Geometry Group (CVG)</a>, <a href="https://ethz.ch/en.html">ETH Zurich</a> as a visiting researcher! </p>
                <p style="font-size:13px"> <strong>[2022.06.09]</strong> One first-author <a href="https://ieeexplore.ieee.org/abstract/document/9810863">paper</a> for unsupervised learning of optical flow estimation is accepted by top journal T-ITS 2022<font color="red"> (IF=9.551)</font>! </p>
                <p style="font-size:13px"> <strong>[2022.06.07]</strong> One first-author <a href="https://ieeexplore.ieee.org/document/9802736">paper</a> for 3D human pose estimation is accepted by TCSD 2022<font color="red"> (IF=4.546)</font>! </p>
                
                <p style="font-size:13px"> <strong>[2022.02.04]</strong> I was awarded <a href="https://www.daad.de/en/the-daad/postdocnet/">the DAAD AInet Fellowship</a>! </p>
                
                <p style="font-size:13px"> <strong>[2022.02.01]</strong> One first-author <a href="https://ieeexplore.ieee.org/abstract/document/9811945">paper</a> for online calibration of camera and LiDAR is accepted by top robotics conference <a href="https://www.icra2022.org/">ICRA 2022</a>! </p>
            <p style="font-size:13px"> <strong>[2021.11.18]</strong> One first-authored <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/aisy.202100197">paper</a> for 3D scene flow is accepted by Advanced Intelligent Systems 2021 <font color="red">(AIS, IF=7.298)</font>！</p> 
            <p style="font-size:13px"> <strong>[2021.10.27]</strong> One first-authored <a href="https://ieeexplore.ieee.org/abstract/document/9606554/">paper</a> for 3D semantic segmentation is accepted by T-cyb 2021 <font color="red">(IF=19.118)</font>！</p> 
            <p style="font-size:13px"> <strong>[2021.09.24]</strong> Got Scholarships for Doctoral Students <font color="red">(Top 1%)</font>! / News: (<a href="https://mp.weixin.qq.com/s/cvWT4Md0G5dIUf_AXc79UQ"><font color="red">Department of
                Automation</font></a>, <a href="https://mp.weixin.qq.com/s/VcBSquX6fakbOjDJ-g5zXg"><font color="red">School of Electronic Information and Electrical Engineering</font></a>) </p>
            <p style="font-size:13px"> <strong>[2021.08.09]</strong> One first-authored <a href="https://ieeexplore.ieee.org/abstract/document/9522122/">paper</a> for 3D action recognition and 3D semantic segmentation is accepted by TIM 2021 <font color="red">(IF=5.332)</font>！</p> 
            <p style="font-size:13px"> <strong>[2021.05.18]</strong> One first-author <a href="https://ieeexplore.ieee.org/abstract/document/9435105/">paper</a> for 3D scene flow is accepted by top journal TIP 2021 <font color="red">(IF=11.041)</font>! </p>
            <p style="font-size:13px"> <strong>[2021.03.01]</strong> One first-author <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_PWCLO-Net_Deep_LiDAR_Odometry_in_3D_Point_Clouds_Using_Hierarchical_CVPR_2021_paper.html">paper</a> for 3D LiDAR odometry is accepted by top computer vision conference <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>! </p>
            <p style="font-size:13px"> <strong>[2021.03.01]</strong> One first-author <a href="https://ieeexplore.ieee.org/abstract/document/9561572">paper</a> for 3D scene flow is accepted by top robotics conference <a href="http://www.icra2021.org/">ICRA 2021</a>! </p>
            <!-- <p style="font-size:13px"> <strong>[2020.11.16]</strong> One co-first-author paper is submitted to CVPR 2021! </p> -->
            <!-- <p style="font-size:13px"> <strong>[2020.10.31]</strong> One co-first-author paper and one second-author paper are submitted to ICRA 2021! </p> -->
        </div></div></td>
       </tr></tbody>
    </table>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading>Publications / Preprints</heading>
          </td>
          </tr></tbody>
    </table>

        <!--SECTION 5 -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>


                <td width="20%"><img src="./imgs/nerf_in_robotics.png" alt="PontTuset" width="180" style="border-style: none"></td>
                <td width="80%" valign="top">
                    <p><a href="https://arxiv.org/abs/2405.01333">
                        <papertitle> NeRF in Robotics: A Survey</papertitle></a>
                        <br><strong>G. Wang*</strong>,  L. Pan*, S. Peng, S. Liu, C. Xu, Y. Miao, W. Zhan, M. Tomizuka, M. Pollefeys, H. Wang 
                        <br>
                        <em>The International Journal of Robotics Research (IJRR)</em>, 2025 <font color="red"><strong>(IF=5.200)</strong></font><br>
                        <a href="https://arxiv.org/abs/2405.01333">arXiv</a>
                    <br>(* indicates equal contributions)
                    <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
                    </p><p></p>
                    <p align="justify" style="font-size:13px">We create this survey to provide a comprehensive understanding of NeRF in the field of robotics. By exploring the advantages and limitations of NeRF, as well as its current applications and future potential, we hope to shed light on this promising area of research. </p>
                </td>
            </tr>


            <td width="20%"><img src="./imgs/I2Pnet.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                <p><a href="https://ieeexplore.ieee.org/abstract/document/11078010/">
                <papertitle> End-to-end 2D-3D Registration between Image and LiDAR Point Cloud for Vehicle Localization
                    </papertitle></a>
                <br> <strong>G. Wang*</strong>, Y. Zheng*, Y. Wu, Y. Guo, Z. Liu, Y. Zhu, W. Burgard, and H. Wang
                <br>
                
                        <em>IEEE Transactions on Robotics (T-RO)</em>, 2025 <font color="red"><strong>(IF=10.500)</strong></font><br>
                <a href="https://ieeexplore.ieee.org/abstract/document/11078010/">IEEE Xplore</a>/
        <a href="https://github.com/IRMVLab/PSFNet"><font color="red">Code</font></a>
        <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=I2PNet&type=star&count=true&size=small"
        frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                <br>(* indicates equal contributions)
                <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
                </p><p></p>
                <p align="justify" style="font-size:13px">we present I2PNet, a novel end-to-end 2D-3D registration network. I2PNet directly
                    registers the raw 3D point cloud with the 2D RGB image using differential modules with a unique
                    target. The 2D-3D cost volume module for differential 2D-3D association is proposed to bridge feature extraction and pose regression. The results demonstrate that I2PNet outperforms the SOTA by a large
                    margin.</p>
            </td>
        </tr>

                <td width="20%"><img src="./imgs/odometry_flow.png" alt="PontTuset" width="180" style="border-style: none"></td>
                <td width="80%" valign="top">
                    <p><a href="https://arxiv.org/abs/2209.04945">
                    <papertitle> Unsupervised Learning of 3D Scene Flow with 3D Odometer Assistance
                        </papertitle></a>
                    <br><strong>G. Wang*</strong>, Z. Feng*, C. Jiang, J. Liu, and H. Wang
                    <br>
                    <em>IEEE Transactions on Intelligent Transportation Systems (T-ITS)</em>, 2025 <font color="red"><strong>(IF=9.551)</strong></font><br>
        <a href="https://arxiv.org/abs/2209.04945"> arXiv</a>/
        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10906337">IEEE Xplore</a>/
        <a href="https://github.com/IRMVLab/PSFNet"><font color="red">Code</font></a>
        <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=PSFNet&type=star&count=true&size=small"
        frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
        <br>
        (* indicates equal contributions)
                    <br>
                    <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
                    </p><p></p>
                    <p align="justify" style="font-size:13px">We propose a novel unsupervised learning method of scene flow with LiDAR odometry, which enables the scene flow network can be trained directly on real-world LiDAR data without scene flow labels.   </p>
                </td>
            </tr>


                <td width="20%"><img src="./imgs/GSRL.gif" alt="PontTuset" width="180" style="border-style: none"></td>
                <td width="80%" valign="top">
                    <p><a href="https://arxiv.org/pdf/2409.20291">
                    <papertitle> RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real
                        Method for Robotic Manipulation Learning</papertitle></a>
                    <br>Y. Wu, L. Pan, W. Wu, <strong>G. Wang</strong>, Y. Miao, H. Wang
                    <br>
                    <em>International Conference on Robotics and Automation (ICRA)</em>, 2025<br>
                    <a href="https://arxiv.org/pdf/2409.20291">arXiv</a>/
                    <!-- <a href="https://ieeexplore.ieee.org/abstract/document/9882014">IEEE Xplore</a>/ -->
                    <a href="https://github.com/IRMV-Manipulation-Group/RL-GSBridge"><font color="red">Code</font></a>
                    <iframe src="https://ghbtns.com/github-btn.html?user=IRMV-Manipulation-Group&repo=RL-GSBridge&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
        
                     <!-- <br>(* indicates equal contributions) -->
                    <br>
                    <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
                    </p><p></p>
                    <p align="justify" style="font-size:13px">We propose RL-GSBridge, a novel real-to-sim-to-real framework which incorporates 3D Gaussian Splatting into the conventional RL simulation pipeline, enabling zero-shot sim-to-real transfer for vision-based deep reinforcement learning.    </p>
                </td>
            </tr>

                


            <td width="20%"><img src="./imgs/segmentation_NIPS.png" alt="PontTuset" width="180" style="border-style: none"></td>
                <td width="80%" valign="top">
                    <p><a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/dc9544b26ad3579477e567588db18cfc-Paper-Conference.pdf">
                    <papertitle> Spherical Frustum Sparse Convolution Network for LiDAR Point Cloud Semantic Segmentation
                        
                       </papertitle></a>
                    <br>Y Zheng*, <strong>G. Wang*</strong>, J Liu, M Pollefeys, H Wang
                    <br>
                    <em> Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024<br>
                    <!-- <a href="https://arxiv.org/abs/2311.11016">arXiv</a>/ -->
                    <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/dc9544b26ad3579477e567588db18cfc-Paper-Conference.pdf">NIPS 2024</a>/
                    <a href="https://github.com/IRMVLab/SFCNet"><font color="red">Code</font></a>
                    <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=SFCNet&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
        
                     <br>(* indicates equal contributions)
                    <br>
                    <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
                    </p><p></p>
                    <p align="justify" style="font-size:13px">To avoid quantized information loss, in this paper, we propose a novel spherical frustum structure, which preserves all points projected onto the same 2D position. Additionally, a hash-based representation is proposed for memory-efficient spherical frustum storage. </p>
                </td>
            </tr>






                <td width="20%"><img src="./imgs/sem_mapping.gif" alt="PontTuset" width="180" style="border-style: none"></td>
                <td width="80%" valign="top">
                    <p><a href="https://arxiv.org/abs/2311.11016">
                    <papertitle> SNI-SLAM: Semantic Neural Implicit SLAM</papertitle></a>
                    <br>S. Zhu*, <strong>G. Wang*</strong>, H. Blum, J. Liu, L. Song, M. Pollefeys, H. Wang 
                    <br>
                    <em>2024 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024<br>
                    <a href="https://arxiv.org/abs/2311.11016">arXiv</a>/
                    <!-- <a href="https://ieeexplore.ieee.org/abstract/document/9882014">IEEE Xplore</a>/ -->
                    <a href="https://github.com/IRMVLab/SNI-SLAM"><font color="red">Code</font></a>
                    <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=SNI-SLAM&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
        
                     <br>(* indicates equal contributions)
                    <br>
                    <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
                    </p><p></p>
                    <p align="justify" style="font-size:13px">We propose a semantic SLAM system utilizing neural implicit representation to achieve high-quality
                        dense semantic mapping and robust tracking. In this system, we integrate appearance, geometry, and
                        semantic features through cross-attention for feature collaboration.   </p>
                </td>
            </tr>
        

                <td width="20%"><img src="./imgs/FLOT_compare.gif" alt="PontTuset" width="180" style="border-style: none"></td>
                <td width="80%" valign="top">
                    <p><a href="https://arxiv.org/abs/2402.18146">
                    <papertitle>3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-labelling</papertitle></a>
                    <br>C. Jiang, <strong>G. Wang</strong>, J. Liu, H. Wang, Z. Ma, Z. Liu, Z. Liang, Y. Shan, D. Du
                    <br>
                    <em>2024 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024<br>
                    <a href="https://arxiv.org/abs/2402.18146">arXiv</a>/
                    <!-- <a href="https://ieeexplore.ieee.org/abstract/document/9882014">IEEE Xplore</a>/ -->
                    <a href="https://github.com/jiangchaokang/3DSFLabelling"><font color="red">Code</font></a>
                    <iframe src="https://ghbtns.com/github-btn.html?user=jiangchaokang&repo=3DSFLabelling&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
    
                     <!-- <br>(* indicates equal contributions) -->
                    <br>
                    <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
                    </p><p></p>
                    <p align="justify" style="font-size:13px">We propose a 3D scene flow pseudo-auto-labelling framework. Given point clouds and
                        initial bounding boxes, both global and local motion parameters are iteratively optimized. Diverse
                        motion patterns are augmented by randomly adjusting these motion parameters, thereby creating a
                        diverse and realistic set of motion labels for the training of 3D scene flow estimation models.  </p>
                </td>
            </tr>

                <td width="20%"><img src="./imgs/pipeline.png" alt="PontTuset" width="180" style="border-style: none"></td>
                <td width="80%" valign="top">
                    <p><a href="https://arxiv.org/abs/2311.17456v1">
                    <papertitle>DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with Diffusion Model </papertitle></a>
                    <br>J. Liu, <strong>G. Wang</strong>, W. Ye, C. Jiang, J. Han, Z. Liu, G. Zhang, D. Du, H. Wang
                    <br>
                    <em>2024 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024<br> <a href="https://arxiv.org/abs/2311.17456v1">arXiv</a>/
                    <!-- <a href="https://ieeexplore.ieee.org/abstract/document/9882014">IEEE Xplore</a>/ -->
                    <a href="https://github.com/IRMVLab/DifFlow3D"><font color="red">Code</font></a>
                    <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=DifFlow3D&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
    
                     <!-- <br>(* indicates equal contributions) -->
                    <br>
                    <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
                    </p><p></p>
                    <p align="justify" style="font-size:13px">To achieve the robust scene flow estimation, we proposed a novel uncertainty-aware scene flow
                        estimation network with the diffusion probabilistic model. Iterative diffusion-based refinement is
                        designed to enhance the correlation robustness and resilience to challenging cases, e.g. dynamics, noisy
                        inputs, repetitive patterns, etc.  </p>
                </td>
            </tr>


            


                <td width="20%"><img src="./imgs/regformer.png" alt="PontTuset" width="180" style="border-style: none"></td>
                <td width="80%" valign="top">
                    <p><a href="https://arxiv.org/abs/2303.12384">
                    <papertitle>RegFormer: An Efficient Projection-Aware Transformer Network for Large-Scale Point Cloud Registration</papertitle></a>
                    <br>J. Liu*, <strong>G. Wang*</strong>, Z. Liu, C. Jiang, M. Pollefeys, H. Wang
                    <br>
                    <em>2023 International Conference on Computer Vision (ICCV)</em>, 2023<br>
                    <a href="https://arxiv.org/abs/2303.12384">arXiv</a>/
                    <!-- <a href="https://ieeexplore.ieee.org/abstract/document/9882014">IEEE Xplore</a>/ -->
                    <a href="https://github.com/IRMVLab/RegFormer"><font color="red">Code</font></a>
                    <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=RegFormer&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
    
                     <br>(* indicates equal contributions)
                    <br>
                    <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
                    </p><p></p>
                    <p align="justify" style="font-size:13px">We propose an end-to-end efficient point cloud registration method of 100,000 level point clouds.  </p>
                </td>
            </tr>


                <td width="20%"><img src="./imgs/defl.gif" alt="PontTuset" width="180" style="border-style: none"></td>
                <td width="80%" valign="top">
                    <p><a href="https://arxiv.org/abs/2308.04383">
                    <papertitle>DELFlow: Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds</papertitle></a>
                    <br>C. Peng*, <strong>G. Wang*</strong>, X. W. Lo, X. Wu, C. Xu, M. Tomizuka, W. Zhan, H. Wang
                    <br>
                    <em>2023 International Conference on Computer Vision (ICCV)</em>, 2023<br>
                    <a href="https://arxiv.org/abs/2308.04383">arXiv</a>/
                    <!-- <a href="https://ieeexplore.ieee.org/abstract/document/9882014">IEEE Xplore</a>/ -->
                    <a href="https://github.com/IRMVLab/DELFlow"><font color="red">Code</font></a>
                    <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=DELFlow&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
    
                     <br>(* indicates equal contributions)
                    <br>
                    <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
                    </p><p></p>
                    <p align="justify" style="font-size:13px">We propose an efficient and high-precision scene flow learning method for large-scale point clouds, achieving the efficiency of the 2D method and the high accuracy of the 3D method.  </p>
                </td>
            </tr>

                <td width="20%"><img src="./imgs/rlsac.gif" alt="PontTuset" width="180" style="border-style: none"></td>
                <td width="80%" valign="top">
                    <p><a href="https://arxiv.org/abs/2308.05318">
                    <papertitle>RLSAC: Reinforcement Learning enhanced Sample Consensus for End-to-End Robust Estimation</papertitle></a>
                    <br>C. Nie*, <strong>G. Wang*</strong>, Z. Liu, L. Cavalli, M. Pollefeys, H. Wang
                    <br>
                    <em>2023 International Conference on Computer Vision (ICCV)</em>, 2023<br>
                    <a href="https://arxiv.org/abs/2308.05318">arXiv</a>/
                    <!-- <a href="https://ieeexplore.ieee.org/abstract/document/9882014">IEEE Xplore</a>/ -->
                    <a href="https://github.com/IRMVLab/RLSAC"><font color="red">Code</font></a>
                    <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=RLSAC&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
    
                     <br>(* indicates equal contributions)
                    <br>
                    <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
                    </p><p></p>
                    <p align="justify" style="font-size:13px"> We model the RANSAC sampling consensus as a reinforcement learning process, achieving a full end-to-end learning sampling consensus robust estimation.  </p>
                </td>
            </tr>



        <td width="20%"><img src="./imgs/20_arxiv_DDPGwB.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://ieeexplore.ieee.org/abstract/document/9882014">
	            <papertitle>Learning of Long-Horizon Sparse-Reward Robotic Manipulator Tasks With Base Controllers</papertitle></a>
                <br><strong>G. Wang*</strong>, M. Xin*, Z. Liu, and H. Wang
                <br>
                <em>IEEE Transactions on Neural Networks and Learning Systems (T-NNLS)</em>, 2022 <font color="red"><strong>(IF=19.118)</strong></font><br>
		        <a href="https://arxiv.org/abs/2011.12105">arXiv</a>/
		        <a href="https://ieeexplore.ieee.org/abstract/document/9882014">IEEE Xplore</a>/
                <a href="https://github.com/IRMVLab/BCLearning"><font color="red">Code</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=BCLearning&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="100px" height="20px"></iframe>

                 <br>(* indicates equal contributions)
                <br>
                <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
                </p><p></p>
			    <p align="justify" style="font-size:13px">We introduce a method of learning challenging sparse-reward tasks utilizing existing controllers. Compared to previous works of learning from demonstrations, our method improves sample efficiency by orders of magnitude and can learn online safely.  </p>
            </td>
        </tr>

    
                <td width="20%"><img src="./imgs/efficient_odometry.png" alt="PontTuset" width="180" style="border-style: none"></td>
                <td width="80%" valign="top">
                    <p><a href="https://ieeexplore.ieee.org/abstract/document/9893384">
                    <papertitle> Efficient 3D Deep LiDAR Odometry</papertitle></a>
                    <br><strong>G. Wang*</strong>, X. Wu*, S. Jiang, Z. Liu, and H. Wang
                    <br>
                    <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2022 <font color="red"><strong>(IF=24.314)</strong></font><br>
                    <a href="https://arxiv.org/abs/2111.02135"> arXiv</a>/
                    <a href="https://ieeexplore.ieee.org/abstract/document/9893384">IEEE Xplore</a>/
                <a href="https://github.com/IRMVLab/EfficientLO-Net"><font color="red">Code</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=EfficientLO-Net&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                    
                    <br>(* indicates equal contributions)
                </p><p></p>
                    <p align="justify" style="font-size:13px">We propose a new efficient 3D point cloud learning method, which is specially designed for the frame-by-frame processing task of real-time perception and localization of robots. It can accelerate the deep LiDAR odometry of our previous CVPR to real-time while improving the accuracy.</p>
                </td>
            </tr>
            
            


            <td width="20%"><img src="./imgs/ECCV.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://link.springer.com/chapter/10.1007/978-3-031-19827-4_3">
                 <papertitle>What Matters for 3D Scene Flow Network </papertitle></a>
                 <br> <strong>G. Wang*</strong>, Y. Hu*, Z. Liu, Y. Zhou, W. Zhan, M. Tomizuka, and H. Wang<br>

                 <a href="https://eccv2022.ecva.net/"> European Conference on Computer Vision  (ECCV), 2022</a><br>
                 <a href="https://arxiv.org/abs/2207.09143">arXiv</a>/
                 <a href="https://link.springer.com/chapter/10.1007/978-3-031-19827-4_3">ECCV 2022</a>/
                 <a href="https://github.com/IRMVLab/3DFlow"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=3DFlow&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                 <!-- <a href="https://arxiv.org/abs/2012.00972">arXiv</a> / -->
                 <!-- <a href="https://github.com/QingyongHu/SpinNet"><font color="red">Code</font></a> -->
                 <!-- <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SpinNet&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe> -->
                 <br>(* indicates equal contributions)
                 <p align="justify" style="font-size:13px">We introduce a novel flow embedding layer with all-to-all mechanism and reverse verification mechanism. Besides,
                    we investigate and compare several design choices in key components of the 3D scene flow network and achieve SOTA performance.
                 </p>
                <p></p>
            </td>
        </tr>



        <td width="20%"><img src="./imgs/20_arxiv_3d_odometry.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_PWCLO-Net_Deep_LiDAR_Odometry_in_3D_Point_Clouds_Using_Hierarchical_CVPR_2021_paper.html">
                 <papertitle>PWCLO-Net: Deep LiDAR Odometry in 3D Point Clouds Using Hierarchical Embedding Mask Optimization</papertitle></a>
                 <br> <strong>G. Wang*</strong>, X. Wu*, Z. Liu, and H. Wang<br>

                 <a href="http://cvpr2021.thecvf.com/"> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021</a>
                 <a href="https://arxiv.org/abs/2012.00972">arXiv</a>/
                 <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_PWCLO-Net_Deep_LiDAR_Odometry_in_3D_Point_Clouds_Using_Hierarchical_CVPR_2021_paper.html">CVPR 2021</a>/
                 <a href="https://github.com/IRMVLab/PWCLONet"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=PWCLONet&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                 <!-- <a href="https://arxiv.org/abs/2012.00972">arXiv</a> / -->
                 <!-- <a href="https://github.com/QingyongHu/SpinNet"><font color="red">Code</font></a> -->
                 <!-- <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SpinNet&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe> -->
                 <br>(* indicates equal contributions)
                 <p align="justify" style="font-size:13px">We introduce a novel 3D point cloud learning model for deep LiDAR odometry, named PWCLO-Net, using hierarchical embedding mask optimization. It outperforms all recent learning-based methods and the geometry-based approach, LOAM with mapping optimization, on most sequences of the KITTI odometry dataset.
                 </p>
                <p></p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/detection.png" alt="PontTuset" width="180" style="border-style: none"></td>
        <td width="80%" valign="top">
            <p><a href="https://arxiv.org/abs/2209.07419">
            <papertitle> FFPA-Net: Efficient Feature Fusion with Projection Awareness for 3D Object Detection
                </papertitle></a>
            <br>C. Jiang*, <strong>G. Wang*</strong>, J. Wu*, Y. Miao, and H. Wang
            <br>
            <a href="https://arxiv.org/abs/2209.07419"> arXiv</a>
            <br>(* indicates equal contributions)
            <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
            </p><p></p>
            <p align="justify" style="font-size:13px">We propose an efficient feature fusion framework with projection awareness for 3D
                Object Detection.</p>
        </td>
    </tr>

        <td width="20%"><img src="./imgs/tracking_video.gif" alt="PontTuset" width="180" style="border-style: none"></td>
        <td width="80%" valign="top">
            <p><a href="https://arxiv.org/abs/2203.16268">
            <papertitle> Interactive Multi-scale Fusion of 2D and 3D
                Features for Multi-object Tracking
                </papertitle></a>
            <br><strong>G. Wang*</strong>, C. Peng*, J. Zhang, and H. Wang
            <br>
        <em>IEEE Transactions on Intelligent Transportation Systems (T-ITS)</em>, 2022 <font color="red"><strong>(IF=9.551)</strong></font><br>
        <a href="https://arxiv.org/abs/2203.16268"> arXiv</a>/
        <a href="https://ieeexplore.ieee.org/abstract/document/10132881">IEEE Xplore</a>/
        <a href="https://github.com/IRMVLab/InterMOT"><font color="red">Code</font></a>
        <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=InterMOT&type=star&count=true&size=small"
        frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
        <br>
        (* indicates equal contributions)
            <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
            </p><p></p>
            <p align="justify" style="font-size:13px">We propose an interactive feature fusion between multi-scale features of images and point clouds. Besides, we explore the effectiveness of pre-training on each single modality and fine-tuning the fusion-based model.</p>
        </td>
    </tr>


        <td width="20%"><img src="./imgs/DetFlowTrack.png" alt="PontTuset" width="180" style="border-style: none"></td>
        <td width="80%" valign="top">
            <p><a href="https://arxiv.org/abs/2203.02157">
            <papertitle> DetFlowTrack: 3D Multi-object Tracking based on Simultaneous Optimization of Object Detection and Scene Flow Estimation
                </papertitle></a>
            <br>Y. Shen, <strong>G. Wang</strong>, and H. Wang
            <br>
            <a href="https://arxiv.org/abs/2203.02157"> arXiv</a>
            <br>
            <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
            </p><p></p>
            <p align="justify" style="font-size:13px">We propose a new joint learning method for 3D object detection and 3D multi-object tracking based on 3D scene flow. </p>
        </td>
    </tr>


        <td width="20%"><img src="./imgs/Residual_Scene_Flow.png" alt="PontTuset" width="180" style="border-style: none"></td>
        <td width="80%" valign="top">
            <p><a href="https://arxiv.org/abs/2109.04685">
            <papertitle> Residual 3D Scene Flow Learning with Context-Aware Feature Extraction</papertitle></a>
            <br><strong>G. Wang*</strong>, Y. Hu*, X. Wu, and H. Wang
            <br>
            <em>IEEE Transactions on Instrumentation and Measurement (TIM)</em>, 2022 <font color="red"><strong>(IF=5.332)</strong></font><br>
            <a href="https://arxiv.org/abs/2109.04685"> arXiv</a>/
            <a href="https://ieeexplore.ieee.org/abstract/document/9754543"> IEEE Xplore</a>/
            <a href="https://github.com/IRMVLab/CARFlow"><font color="red">Code</font></a>
            <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=CARFlow&type=star&count=true&size=small"
            frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
            <br>(* indicates equal contributions)
            <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
            </p><p></p>
            <p align="justify" style="font-size:13px">We propose a novel context-aware set conv layer to cope with repetitive patterns in the learning of 3D scene flow. We also propose an explicit residual flow learning structure in the residual flow refinement layer to cope with long-distance movement. </p>
        </td>
    </tr>


    <td width="20%"><img src="./imgs/mul_posenet.png" alt="PontTuset" width="180" style="border-style: none"></td>
    <td width="80%" valign="top">
        <p><a href="https://ieeexplore.ieee.org/abstract/document/9924173">
        <papertitle> 3D Hierarchical Refinement and Augmentation for Unsupervised Learning of Depth and Pose from Monocular Video</papertitle></a>
        <br><strong>G. Wang*</strong>, J. Zhong*, S. Zhao, W. Wu, Z. Liu, and H. Wang
        <br>
            <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em>, 2022 <font color="red"><strong>(IF=5.859)</strong></font><br>
            <a href="https://arxiv.org/abs/2112.03045"> arXiv</a>/
            <a href="https://ieeexplore.ieee.org/abstract/document/9924173"> IEEE Xplore</a>/
            <a href="https://github.com/IRMVLab/HRANet"><font color="red">Code</font></a>
            <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=HRANet&type=star&count=true&size=small"
            frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
            <br>(* indicates equal contributions)
        <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
        </p><p></p>
        <p align="justify" style="font-size:13px">We propose a novel unsupervised training framework of depth and pose with 3D hierarchical refinement and augmentation using explicit 3D geometry. </p>
    </td>
</tr>


<td width="20%"><img src="./imgs/fusionnet.png" alt="PontTuset" width="180" style="border-style: none"></td>
<td width="80%" valign="top">
    <p><a href="https://ieeexplore.ieee.org/abstract/document/9811945">
    <papertitle>FusionNet: Coarse-to-Fine Extrinsic Calibration Network of LiDAR and Camera with Hierarchical Point-pixel Fusion</papertitle></a>
    <br><strong>G. Wang*</strong>, J. Qiu*, Y. Guo*, and H. Wang<br>
    
    <a href="https://www.icra2022.org/">International Conference on Robotics and Automation (ICRA), Xi'an, China, 2021. </a>
    <br>
    <a href="https://ieeexplore.ieee.org/abstract/document/9811945"> IEEE Xplore</a>
    <br>(* indicates equal contributions)

    </p><p></p>
    <p align="justify" style="font-size:13px">We propose Fusion-Net, an online and end-to-end solution that can automatically detect and correct the extrinsic calibration matrix between LiDAR and a monocular RGB camera without any specially
        designed targets or environments. </p>
</td>
</tr>


    <td width="20%"><img src="./imgs/SFGAN.png" alt="PontTuset" width="180" style="border-style: none"></td>
    <td width="80%" valign="top">
        <p><a href="https://onlinelibrary.wiley.com/doi/full/10.1002/aisy.202100197">
        <papertitle> SFGAN: Unsupervised Generative Adversarial Learning of 3D Scene Flow from the 3D Scene Self</papertitle></a>
        <br><strong>G. Wang</strong>, C. Jiang, Z. Shen, Y. Miao, and H. Wang
        <br>
        <a href="https://onlinelibrary.wiley.com/page/journal/26404567/homepage/productinformation.html">Advanced Intelligent Systems (AIS)</a>, 2021 <font color="red"><strong>(AIS, IF=7.298)</strong></font><br>
                 <a href="https://www.authorea.com/doi/full/10.22541/au.163335790.03073492">authorea</a>/
                 <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/aisy.202100197"> Wiley Online Library</a>
        <br>
        <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
        </p><p></p>
        <p align="justify" style="font-size:13px">We utilize the generative adversarial networks (GAN) to self-learn 3D scene flow without ground truth. </p>
    </td>
</tr>


        <td width="20%"><img src="./imgs/20_arxiv_un_scene_flow.png" alt="PontTuset" width="180" style="border-style: none"></td>
        <td width="80%" valign="top">
            <p><a href="https://ieeexplore.ieee.org/abstract/document/9561572">
            <papertitle>Unsupervised Learning of Scene Flow from Monocular Camera</papertitle></a>
            <br><strong>G. Wang*</strong>, X. Tian*, R. Ding, and H. Wang<br>
            
            <a href="http://www.icra2021.org/">International Conference on Robotics and Automation (ICRA), Xi'an, China, 2021. </a>
            <br>
            <a href="https://arxiv.org/abs/2206.03673">arXiv</a>/
            <a href="https://ieeexplore.ieee.org/abstract/document/9561572"> IEEE Xplore</a>
            <br>(* indicates equal contributions)

            </p><p></p>
            <p align="justify" style="font-size:13px">We present a framework to realize the unsupervised learning of scene flow from a monocular camera. </p>
        </td>
    </tr>

        <td width="20%"><img src="./imgs/20_arxiv_ASTAConv.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://ieeexplore.ieee.org/abstract/document/9522122/">
                 <papertitle>Anchor-Based Spatio-Temporal Attention 3D Convolutional Networks for Dynamic 3D Point Cloud Sequences</papertitle></a>
                 <br><strong>G. Wang</strong>, H. Liu, M. Chen, Y. Yang, Z. Liu, and H. Wang<br>
                 <em>IEEE Transactions on Instrumentation and Measurement (TIM)</em>, 2021 <font color="red"><strong>(IF=5.332)</strong></font><br>
                 <a href="https://arxiv.org/abs/2012.10860">arXiv</a>/
                 <a href="https://ieeexplore.ieee.org/abstract/document/9522122/"> IEEE Xplore</a>/
                 <a href="https://github.com/IRMVLab/ASTA3DConv"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=ASTA3DConv&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                 <br>
                  <!--<font color="red"><strong>..</strong></font><br>-->
                 <!-- <a href="http://arxiv.org/abs/2009.03137">arXiv</a> /
                 <a href="https://www.youtube.com/watch?v=IG0tTdqB3L8">Demo</a> /
                 <a href="https://github.com/QingyongHu/SensatUrban"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SensatUrban&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe> -->
                <!-- <br>(* indicates corresponding author) -->
                <!-- <br>(* indicates equal contributions) -->
                <p align="justify" style="font-size:13px">We introduce an Anchor-based Spatial-Temporal Attention Convolution operation (ASTAConv) to process dynamic 3D point cloud sequences. It makes better use of the structured information within the local region and learns spatial-temporal embedding features from dynamic 3D point cloud sequences.
                  </p>
                <p></p>
            </td>
        </tr>


                <td width="20%"><img src="./imgs/20_arxiv_HAFlowNet.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://ieeexplore.ieee.org/abstract/document/9435105/">
                 <papertitle>Hierarchical Attention Learning of Scene Flow in 3D Point Clouds</papertitle></a>
                 <br><strong>G. Wang*</strong>, X. Wu*, Z. Liu, and H. Wang<br>
		<em>IEEE Transactions on Image Processing (TIP)</em>, 2021 <font color="red"><strong>(IF=11.041)</strong></font><br>
		        <a href="https://arxiv.org/abs/2010.05762">arXiv</a>/
		        <a href="https://ieeexplore.ieee.org/abstract/document/9435105/">IEEE Xplore</a>/
                <a href="https://github.com/IRMVLab/HALFlow"><font color="red">Code</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=HALFlow&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                 <br>
                  <!--<font color="red"><strong>..</strong></font><br>-->
                (* indicates equal contributions)
                <p align="justify" style="font-size:13px">We introduce a novel hierarchical neural network with double attention for learning the correlation of point features in adjacent frames and refining scene flow from coarse to fine layer by layer. It has a new, more-for-less hierarchical architecture. The proposed network achieves the state-of-the-art performance of 3D scene flow estimation on the FlyingThings3D and KITTI Scene Flow 2015 datasets.
                  </p>
                <p></p>
            </td>
        </tr>



    <td width="20%"><img src="./imgs/nccflow.png" alt="PontTuset" width="180" style="border-style: none"></td>
    <td width="80%" valign="top">
        <p><a href="https://arxiv.org/abs/2107.03610">
        <papertitle> NccFlow: Unsupervised Learning of Optical Flow With Non-occlusion from Geometry</papertitle></a>
        <br><strong>G. Wang*</strong>, S. Ren*, and H. Wang
        <br>
        <em>IEEE Transactions on Intelligent Transportation Systems (T-ITS)</em>, 2022 <font color="red"><strong>(IF=9.551)</strong></font><br>
        <a href="https://arxiv.org/abs/2107.03610"> arXiv</a>/
        <a href="https://ieeexplore.ieee.org/abstract/document/9810863">IEEE Xplore</a>/
        <a href="https://github.com/IRMVLab/NccFlow"><font color="red">Code</font></a>
        <iframe src="https://ghbtns.com/github-btn.html?user=IRMVLab&repo=NccFlow&type=star&count=true&size=small"
        frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
        <br>
        (* indicates equal contributions)
        <!-- Submitted to 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), under review.  -->
        </p><p></p>
        <p align="justify" style="font-size:13px">We introduce a novel unsupervised learning method of optical flow by considering the constraints in non-occlusion regions with geometry analysis. </p>
    </td>
</tr>

    <td width="20%"><img src="./imgs/3D_human_pose.gif" alt="PontTuset" width="180" style="border-style: none"></td>
    <td width="80%" valign="top">
        <p><a href="https://arxiv.org/abs/2106.14706">
        <papertitle> Motion Projection Consistency Based 3D Human Pose Estimation with Virtual Bones from Monocular Videos</papertitle></a>
        <br><strong>G. Wang*</strong>, H. Zeng*, Z. Wang, Z. Liu, and H. Wang
        <br>
        <em>IEEE Transactions on Cognitive and Developmental Systems (TCSD)</em>, 2022 <font color="red"><strong>(IF=4.546)</strong></font><br>
        <a href="https://arxiv.org/abs/2106.14706"> arXiv</a>/
        <a href="https://ieeexplore.ieee.org/document/9802736"> IEEE Xplore</a>
        <br>(* indicates equal contributions)
        </p><p></p>
        <p align="justify" style="font-size:13px">We introduce a novel unsupervised learning method of the 3D human pose by considering the loop constraints from real/virtual bones and the joint motion constraints in consecutive frames. </p>
    </td>
</tr>

<td width="20%"><img src="./imgs/arxiv_Spherical_ Interpolated.gif" alt="PontTuset" width="180" style="border-style: none"></td>
<td width="80%" valign="top">
     <p><a href="https://arxiv.org/abs/2011.13784">
     <papertitle>Spherical Interpolated Convolutional Network with Distance-Feature Density for 3D Semantic Segmentation of Point Clouds</papertitle></a>
     <br><strong>G. Wang</strong>, Y. Yang, Z. Liu, and H. Wang <br>
		<em>IEEE Transactions on Cybernetics (T-Cyb)</em>, 2021 <font color="red"><strong>(IF=19.118)</strong></font><br>
		        <a href="https://arxiv.org/abs/2011.13784">arXiv</a>/
		        <a href="https://ieeexplore.ieee.org/abstract/document/9606554/">IEEE Xplore</a>
                 <br>
     <!-- / -->
     <!-- <a href="https://github.com/alextrevithick/GRF"><font color="red">Code</font></a> -->
     <!-- <iframe src="https://ghbtns.com/github-btn.html?user=alextrevithick&repo=GRF&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe> -->
     
     <p align="justify" style="font-size:13px">We introduce a spherical interpolated convolution operator to replace the traditional grid-shaped 3D convolution operator. It improves the accuracy and reduces the parameters of the network.
     </p>
    <p></p>
</td>
</tr>



	    <td width="20%"><img src="./imgs/DOPlearning.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                <p><a href="https://ieeexplore.ieee.org/document/9152137">
	            <papertitle>Unsupervised Learning of Depth, Optical Flow and Pose with Occlusion From 3D Geometry</papertitle></a>
                <br><strong>G. Wang</strong>, C. Zhang, H. Wang, J. Wang, Y. Wang, and X. Wang<br>
                <em>IEEE Transactions on Intelligent Transportation Systems (T-ITS)</em>, 2020 <font color="red"><strong>(IF=9.551)</strong></font><br>
		        <a href="https://arxiv.org/abs/2003.00766">arXiv</a>/
		        <a href="https://ieeexplore.ieee.org/document/9152137">IEEE Xplore</a>/
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/NXr2-AOrl_VllIs_2buLwA"><font color="red">(DeepBlue深兰科技,</font></a>
                <a href="https://ifosa.org/invited.html"><font color="red">The First International Forum on 3D Optical Sensing and Applications (iFOSA 2020),</font></a>
                <!-- <a href="https://mp.weixin.qq.com/s/C1FDPkAkmnmAZ_gvvtzBHw"><font color="red">AI科技评论,</font></a>
                <a href="https://mp.weixin.qq.com/s/wViZITtsb4j3oFtOpJI9wQ"><font color="red">将门创投,</font></a>
                <a href="https://mp.weixin.qq.com/s/S7mHrOxOwTIhDGPhu1SI4A"><font color="red">CVer,</font></a> -->
                <a href="https://mp.weixin.qq.com/s/GaS2F-2-aEQIJ-6ZLahynQ"><font color="red">计算机视觉life)</font>/</a>
                <a href="https://www.youtube.com/watch?v=Y8Up9OsZbcg"><font color="red">Video</font></a>/
                <a href="https://github.com/Yang7879/AttSets"><font color="red">Code</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=guangmingw&repo=DOPlearning&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose a method to explicitly handle occlusion, propose the less-than-mean mask, the maximum normalization, and the consistency of depth-pose and optical flow in the occlusion regions. </p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/occlusion_mask.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://ieeexplore.ieee.org/abstract/document/8793622">
	             <papertitle>Unsupervised Learning of Monocular Depth and Ego-Motion Using Multiple Masks</papertitle></a>
                 <br><strong>G. Wang</strong>, H. Wang, Y. Liu, and W. Chen
                 <br>
                 <em>International Conference on Robotics and Automation (ICRA)</em>, Montreal, Canada, 2019 
                 <br>
                 <!--<font color="red"><strong>..</strong></font><br>-->
                 <a href="https://arxiv.org/abs/2104.00431">arXiv</a>/
                 <a href="https://ieeexplore.ieee.org/abstract/document/8793622">IEEE Xplore</a> /
                 <!-- <a href="http://kaldir.vc.in.tum.de/scannet_benchmark/result_details?id=118">ScanNet Benchmark</a> / -->
                 <!-- <a href="https://www.reddit.com/r/MachineLearning/comments/bx8jhz/r_new_sota_for_3d_object_detection/">Reddit Discussion</a> / -->
                 <font color="red"> News:</font>
                 <a href="https://mp.weixin.qq.com/s/sKSb-dLRTifiwK9hcx_Vcg"><font color="red">(泡泡机器人,</font></a>
                 <!-- <a href="https://mp.weixin.qq.com/s/4GPkmTri4Vk7Xy0J8TiBNw"><font color="red">图像算法,</font></a>
                 <a href="https://mp.weixin.qq.com/s/C1FDPkAkmnmAZ_gvvtzBHw"><font color="red">AI科技评论,</font></a>
                 <a href="https://mp.weixin.qq.com/s/wViZITtsb4j3oFtOpJI9wQ"><font color="red">将门创投,</font></a>
                 <a href="https://mp.weixin.qq.com/s/S7mHrOxOwTIhDGPhu1SI4A"><font color="red">CVer,</font></a> -->
                 <a href="https://mp.weixin.qq.com/s/-IcQKYpXcs6CPme8d7Jbww"><font color="red">上海交大研究生教育)</font>/</a>
                 <a href="https://ieeexplore.ieee.org/ielx7/8780387/8793254/8793622/2533_MM.zip?tp=&arnumber=8793622"><font color="red">Video</font></a>/
                 <a href="https://github.com/guangmingw/DOPlearning"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=guangmingw&repo=DOPlearning&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
			     <p align="justify" style="font-size:13px">We propose a new unsupervised learning method of depth and ego motion using multiple masks to handle the occlusion problem.</p>
                <p></p>
            </td>
        </tr>



	   
        </tbody>
    </table>


        <!--SECTION 6 -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
               <td><heading>Academic Services</heading>
                <p style="font-size:13px"> <strong>Associate Editor (AE)</strong>  for the <strong>IEEE Robotics and Automation Letters (RA-L, IF=4.6)</strong> Editorial Board in the area Visual Perception and Learning
                   <p style="font-size:13px"> <strong>Associate Editor (AE)</strong> for the Conference Editorial Board (CEB) of the IEEE Robotics and Automation Society for <strong>International Conference on Robotics and Automation (ICRA) 2024, 2025</strong></p>
                   <p style="font-size:13px"> <strong>Associate Editor (AE)</strong> for the Conference Editorial Board (CEB) of the IEEE Robotics and Automation Society for <strong>International Conference on Intelligent Robots and Systems (IROS) 2024, 2025</strong></p>
                   <p style="font-size:13px"> <strong>Organizer</strong> of Workshop “3D Reconstruction Technology and Applications” at University of Cambridge</p>
                   <p style="font-size:13px"> <strong>IEEE Member</strong></p>
                   <p style="font-size:13px"> <strong>IEEE Robotics and Automation Society Member</strong></p>
                   <p style="font-size:13px"> <strong>IEEE Young Professionals Member</strong></p>    
                <!-- <p style="font-size:13px"> <strong>[2020.04]</strong> Invited talk about 3D reconstruction at Sun Yat-sen University, hosted by <a href="http://yulanguo.me/">Prof. Yulan Guo</a>.</p> -->
                <!-- <p style="font-size:13px"> <strong>Reviewer for journals:</strong> IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), International Journal of Computer Vision (IJCV), IEEE Transactions on Image Processing (T-IP), IEEE Transactions on Intelligent Transportation Systems (T-ITS), IEEE Transactions on Cybernetics (T-cyb), IEEE Transactions on Systems, Man and Cybernetics: Systems (T-SMC), IEEE Transactions on Neural Networks and Learning Systems (T-NNLS), Transactions on Intelligent Transportation Systems (T-ITS), IEEE/ASME Transactions on Mechatronics (T-Mech), IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT), IEEE Transactions on Medical Imaging (T-MI), IEEE Transactions on Automation Science and Engineering (T-ASE), IEEE Transactions on Industrial Informatics (T-II), 	IEEE Transactions on Industrial Electronics (T-IE), IEEE Transactions on Intelligent Vehicles (T-IV), IEEE Transactions on Vehicular Technology (T-VT), IEEE Robotics and Automation Letters (RAL), IEEE Transactions on Broadcasting (T-BC), IEEE Transactions on Cognitive and Developmental Systems (T-CDS), 	
                    IEEE Transactions on Artificial Intelligence (T-AI), IEEE Transactions on Emerging Topics in Computational Intelligence (T-ETCI), IEEE Transactions on Medical Robotics and Bionics (T-MRB), Pattern Recognition (PR), Machine Intelligence Research (MIR), IEEE Internet of Things Journal (IoT-J), Neurocomputing, Complex & Intelligent Systems (CAIS), International Journal of Humanoid Robotics (IJHR), Geo-spatial Information Science (GIS), International Journal of Social Robotics (IJSR).</p> -->
                    <p style="font-size:13px"> <strong>Reviewer for journals:</strong> Nature, IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), International Journal of Computer Vision (IJCV), IEEE Transactions on Image Processing (T-IP), IEEE Transactions on Intelligent Transportation Systems (T-ITS), IEEE Transactions on Cybernetics (T-cyb), IEEE Transactions on Systems, Man and Cybernetics: Systems (T-SMC), IEEE Transactions on Neural Networks and Learning Systems (T-NNLS), Transactions on Intelligent Transportation Systems (T-ITS), IEEE/ASME Transactions on Mechatronics (T-Mech), IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT), IEEE Transactions on Medical Imaging (T-MI), IEEE Transactions on Automation Science and Engineering (T-ASE), IEEE Transactions on Industrial Informatics (T-II), 	IEEE Transactions on Industrial Electronics (T-IE), IEEE Transactions on Intelligent Vehicles (T-IV), IEEE Transactions on Vehicular Technology (T-VT), IEEE Transactions on Broadcasting (T-BC), IEEE Transactions on Cognitive and Developmental Systems (T-CDS), 	
                        IEEE Transactions on Artificial Intelligence (T-AI), IEEE Transactions on Emerging Topics in Computational Intelligence (T-ETCI), IEEE Transactions on Medical Robotics and Bionics (T-MRB), IEEE Robotics and Automation Letters (RAL), Pattern Recognition (PR), Machine Intelligence Research (MIR).</p>
                    
                    <p style="font-size:13px"> <strong>Reviewer for conferences:</strong> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE International Conference on Computer Vision (ICCV), European Conference on Computer Vision (ECCV), AAAI Conference on Artificial Intelligence (AAAI), International Conference on Robotics and Automation (ICRA), International Conference on Intelligent Robots and Systems (IROS), European Conference on Mobile Robots (ECMR).</p>
               
            </td>
               </tr></tbody>
       </table>

    <!--SECTION 6 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Talks & Posters</heading>
                <p style="font-size:13px"> <strong>[2024.12]</strong> Conference Poster on “Spherical Frustum Sparse Convolution Network for LiDAR Point Cloud Semantic Segmentation,” at <a href="https://neurips.cc/Conferences/2024">NIPS 2024 </a> in Vancouver, Canada, December 10-15, 2024.</p>
                
                <p style="font-size:13px"> <strong>[2024.7]</strong> Conference Poster on “Dense 3D Neural Map Reconstruction Only Using a Low-cost LiDAR,” at <a href="https://www.cmu.edu/cee/i3ce2024/index.html">I3CE 2024</a> in
                    Pittsburgh, Pennsylvania, USA, July 28-31, 2024.</p>
                
                <p style="font-size:13px"> <strong>[2023.10]</strong> Conference Poster on “DELFlow: Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds,” at <a href="https://iccv2023.thecvf.com/">ICCV 2023 </a> in Paris, France, October 2-6, 2023.</p>
                
                <p style="font-size:13px"> <strong>[2023.05]</strong> Conference Poster on “Self-supervised Multi-frame Monocular Depth Estimation with Pseudo-LiDAR Pose Enhancement,” at <a href="https://www.icra2023.org/">ICRA 2023 </a> in ExCeL London UK, 29 May–2 June 2023.</p>
                
                <p style="font-size:13px"> <strong>[2023.02]</strong> Conference Poster on “TransLO: A Window-based Masked Point Transformer Framework for Large-scale LiDAR Odometry,” at AAAI 2023 in Washington DC, USA, February 7-14, 2023.
                <p style="font-size:13px"> <strong>[2022.10]</strong> Conference Poster on “What Matters for 3D Scene Flow Network,” at <a href="https://eccv2022.ecva.net/">ECCV 2022 </a> in Tel Aviv, Israel, October 23-27, 2022.</p>
                
                <p style="font-size:13px"> <strong>[2022.08]</strong> Zhidx Open Talk on <a href="https://course.zhidx.com/c/YWFiNjYyNGRjNDY2ZTQ5Njk0MzM=">“3D Scene Flow and Lidar Point Cloud Odometry in Autonomous Driving”</a>, Online.</p>    
                <p style="font-size:13px"> <strong>[2021.06]</strong> Conference Talk on “FusionNet: Coarse-to-Fine Extrinsic Calibration Network of LiDAR and Camera with Hierarchical Point-pixel Fusion” at  <a href="https://www.icra2022.org/">ICRA 2022</a>  in May 23-27, 2022 (Online Talk, the conference is held in Philadelphia (PA), USA).</p>
                
                <p style="font-size:13px"> <strong>[2021.06]</strong> Conference Poster on “PWCLO-Net: Deep LiDAR Odometry in 3D Point Clouds Using Hierarchical Embedding Mask Optimization” at <a href="http://cvpr2021.thecvf.com/">CVPR 2021 </a> in CVPR Virtual, June 19-25, 2021.</p>
                <p style="font-size:13px"> <strong>[2021.05]</strong> Conference Talk on “Unsupervised Learning of Scene Flow from Monocular Camera” at <a href="http://www.icra2021.org/"> ICRA 2021</a> in Xi’an, China, 30 May - 5 June, 2021. </p>
                <p style="font-size:13px"> <strong>[2020.10]</strong> Invited Talk on “Unsupervised Learning of Depth, Optical Flow and Pose with Occlusion From 3D Geometry” at <a href="https://ifosa.org/invited.html"> the First International Forum on 3D Optical Sensing and Applications (iFOSA 2020)</a> held in Being, China, October 17-18, 2020.</p>
                <p style="font-size:13px"> <strong>[2019.07]</strong> Seminar Talk on “Unsupervised Learning of Monocular Depth and Ego-Motion Using Multiple Masks” at <a href="./imgs/SEEEP.mp4">Sino-European Engineering Education Platform (SEEEP) Doctoral Summer School</a> in Instituto Superior Técnico (IST), Lisbon, Portugal, July 22-25, 2019.</p>
                <p style="font-size:13px"> <strong>[2019.05]</strong> Conference Poster on “Unsupervised Learning of Monocular Depth and Ego-Motion Using Multiple Masks” at <a href="https://mp.weixin.qq.com/s/-IcQKYpXcs6CPme8d7Jbww">ICRA 2019</a> in Montreal, Canada, May 20-24, 2019.</p>    
             
             
                
             
             
                <!-- <p style="font-size:13px"> <strong>[2020.04]</strong> Invited talk about 3D reconstruction at Sun Yat-sen University, hosted by <a href="http://yulanguo.me/">Prof. Yulan Guo</a>.</p> -->
             </td>
            </tr></tbody>
    </table>





    <!--SECTION 6 -->
    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Teaching</heading>
             <p> <strong>Nov. 2019 - Jun. 2020</strong>: &ensp;&ensp;  Co-direct four undergraduate graduation projects of SJTU, one of which was submitted to <a href="https://arxiv.org/abs/2011.12105">ICRA 2021</a>.  (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Jul. 2020 - Aug. 2020</strong>: &ensp;&ensp;  Co-direct seven junior undergraduates in summer laboratory practice.  (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Mar. 2020 - Mar. 2021</strong>: &ensp;&ensp; Co-direct four undergraduate innovation projects, two of which were approved by the Shanghai Innovation Training Program for College Students. Some students of them cooperated with me to submit <a href="https://arxiv.org/abs/2010.05762"> a TIP paper</a>, <a href="https://arxiv.org/abs/2012.10860">a T-IM paper</a>, <a href="https://arxiv.org/abs/2011.13784">a T-Cyb paper</a>. (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Nov. 2020 - Jun. 2021</strong>: &ensp;&ensp; Co-direct two undergraduate graduation projects of SJTU, one of which submitted <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_PWCLO-Net_Deep_LiDAR_Odometry_in_3D_Point_Clouds_Using_Hierarchical_CVPR_2021_paper.html"> a CVPR paper</a>.
             <p> <strong>Sep. 2020 - Jun. 2023</strong>: &ensp;&ensp; Co-direct three masters and a doctor at China University of Mining and Technology because my advisor is an adjunct professor there. One of them submitted a T-II paper. One of them submitted an IJCAI2022 paper.  (<a href="http://global.cumt.edu.cn/">The China University of Mining and Technology, CUMT</a>).  
             <p> <strong>Mar. 2021 - Mar. 2022</strong>: &ensp;&ensp; Co-direct five undergraduate innovation projects. Some students cooperated with me to submit a RAL paper and two ICRA papers. (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Nov. 2021 - Nov. 2022</strong>: &ensp;&ensp;	Co-direct three undergraduate innovation projects. Some of their students cooperated with me to submit a T-NNLS paper. (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Nov. 2021 - Jun. 2022</strong>: &ensp;&ensp;	Co-direct ten undergraduate graduation projects of SJTU. (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Nov. 2021 - Jun. 2023</strong>: &ensp;&ensp;	Co-direct one more master in China University of Mining and Technology because my advisor is an adjunct professor there. (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Mar. 2022 - Mar. 2023</strong>: &ensp;&ensp;	Co-direct four undergraduate innovation projects. (co-supervised with Ph.D. student Tianchen Deng.) (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
             <p> <strong>Mar. 2022 - Jun. 2023</strong>: &ensp;&ensp;	Co-direct three PhD students of our laboratory and six masters who are about to enter the laboratories of China University of Mining and Technology. (<a href="http://global.cumt.edu.cn/">The China University of Mining and Technology, CUMT</a>).   </p>
             <p> <strong>Oct. 2022 - Jun. 2023</strong>: &ensp;&ensp;	Co-lead deep learning group with 23 graduate students, including 6 PhD students, 14 master students, 2 accepted PhD students, and 1 accepted master student, in IRMV lab in SJTU. (<a href="http://en.sjtu.edu.cn/">The Shanghai Jiao Tong University, SJTU</a>). </p>
            
            </p>
            </td>
            </tr></tbody>
    </table> -->

<!-- SECTION 7 -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
       <td><heading>Teaching</heading>
        <p style="font-size:13px"> <strong>[2025.03]</strong> Guest Lecturer for Part II ER: Extended Reality at the University of Cambridge. (Invited by <a href="https://www.cl.cam.ac.uk/~fz261/">Dr. Fangcheng Zhong</a>)</p>
        
        <p style="font-size:13px"> <strong>[2024.11]</strong> Guest Lecturer for CS345/912 Sensor Networks and Mobile Data Communications at the University of
            Warwick. (Invited by <a href="https://warwick.ac.uk/fac/sci/dcs/people/yu_guan/">Prof. Yu Guan</a>)</p>
            <p style="font-size:13px"> <strong>[2024.11]</strong> Guest Lecturer for IOTA5201 Reinforcement Learning for Intelligent Decision Making in CyberPhysical Systems at Hong Kong University of Science and Technology.(Invited by <a href="https://facultyprofiles.hkust-gz.edu.cn/faculty-personal-page/YU-Jiadong/jiadongyu">Dr. Jiadong Yu</a>)</p>
        <p style="font-size:13px"> <strong>[2024.09-present]</strong> Supervisor for undergraduates in Downing College, at the University of Cambridge.</p>
        <p style="font-size:13px"> <strong>[2024.09-2024.12]</strong> Supervisor for undergraduates in Clare College, at the University of Cambridge.</p>
        <p style="font-size:13px"> <strong>[2024.09-present]</strong> Teaching Assistant, Demonstrator for IB Integrated Coursework A2 for undergraduates in University of Cambridge.</p>
        <p style="font-size:13px"> <strong>[2024.04-2024.12]</strong> Teaching
            Assistant for the Twin Systems (Digital Twins) Teaching Module at the University of Cambridge (Assistant in designing the course and slides).</p>
            <p style="font-size:13px"> <strong>[2023.10]</strong> <a href="https://mp.weixin.qq.com/s/k3g9zt9jCRbl30prH7YXGw">Lecture on academic career planning </a> for Shanghai Jiao Tong University's "Academic Navigation" training program for MPhil students and PhD students, Autumn 2023.</p>
            <p style="font-size:13px"> <strong>[2023.03-06]</strong> Teaching Assistant (<a href="https://cvg.ethz.ch/teaching/3dvision/projects/3DV_Projects_2023.pdf">Project Supervisor of Masters</a>), 3D Vision in ETH, Spring 2023.</p>
        
        <p style="font-size:13px"> <strong>[2022.10]</strong> delivered a cutting-edge lecture on the topic of “Learning-Based Robot Perception and
            Localization” in the Lectures on Frontier Academics in China University of Mining and Technology, Autumn 2022.</p>
        
        
         </td>
    </tr></tbody>
 </table>



    <!-- SECTION 7 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>Mentoring</heading>
            <!-- <p>
            <strong>Doctor</strong>:
            <br>
               <strong>Chichao Cheng</strong> (SJTU), <strong>Tianchen Deng</strong> (SJTU), <strong>Mingxi Zhuang</strong> (SJTU), <strong>Chang Nie</strong> (SJTU)<br> 
               <strong>Ran Bu</strong> (SJTU), <strong>Jun Ye</strong> (SJTU), <strong>Yizhou Wang</strong> (CUMT), <strong>Siting Zhu</strong> (SJTU)<br> 


            <p>
            <strong>Masters</strong>:
            <br>
            <strong>Huixin Zhang</strong> (SJTU), <strong>Zehang Shen</strong> (SJTU), <strong>Shuaiqi Ren</strong> (SJTU), <strong>Zhiheng Feng</strong> (SJTU)<br> 
            <strong>Jiuming Liu</strong> (SJTU), <strong>Itana Bulatovic</strong> (SJTU), <strong>JiaJie Jacques Xu</strong> (SJTU), <strong>Yangyi Xiao</strong> (SJTU)<br> 
            <strong>JingYun Fang</strong> (SJTU), <strong>Chaokang Jiang</strong> (CUMT), <strong>Huiying Deng</strong> (CUMT), <strong>Xiaolin Wang</strong> (CUMT)<br>
            <strong>Liang Song</strong> (CUMT), <strong>Lei Pan</strong> (CUMT), <strong>Qirong Liu</strong> (CUMT), <strong>Yiqing Xu</strong> (CUMT)
              <p>
              <strong>Undergrad Interns</strong>:
              <br>
              <strong>Shuyang Jiang</strong> (SJTU), <strong> Wenlong Yi</strong> (SJTU), <strong>Chensheng Peng</strong> (SJTU), <strong>Jianwan Luo</strong> (SJTU)<br>
              <strong>Wenhua Wu</strong> (SJTU), <strong>Xingyao Han</strong> (SJTU), <strong>Qingyu Wang</strong> (SJTU), <strong> Yuepeng Zhang</strong> (SJTU)<br>
              <strong>Yu Zheng</strong> (SJTU), <strong>Yufei Luo</strong> (SJTU), <strong>Lianting Huang</strong> (SJTU), <strong>Jiahao Qiu</strong> (SJTU)<br>
               <strong>Yizhe Liu</strong> (SJTU), <strong> Shijie Zhao</strong> (SJTU), <strong>Leiheng Wang</strong> (SJTU), <strong>Pan Li</strong> (SJTU), <strong>Chensi He</strong> (SJTU) <br>
              <strong>Haoxu Huang</strong> (SJTU),  <strong>Wenhui Pei</strong> (SJTU), <strong>Jinyu Zhang</strong> (SJTU), <strong>Qi Wang</strong> (SJTU)
            </p> -->
              
            <p> 
                <strong>Past Masters</strong>:
                <br>
                <strong>Yueling Shen</strong>: &ensp;&ensp; Shanghai Jiao Tong University, SJTU, gone to PlusAI, Inc. to work.<br>
                <strong>Chaokang Jiang</strong>: &ensp;&ensp; CUMT and SJTU, gone to PhiGent Robotics to work.<br>
                <strong>Huiying Deng</strong>: &ensp;&ensp; CUMT and SJTU, gone to China Mobile Communications Group Co., Ltd to work.<br>
                
            </p>   


              <p> 
              <strong>Past Undergrad Interns</strong>:
              <br>
              <strong>Minjian Xin</strong>: &ensp;&ensp; SJTU, gone to University of California at San Diego, UCSD to pursue master. <br>             
              <strong>Xinrui Wu</strong>: &ensp;&ensp; Shanghai Jiao Tong University, SJTU, gone to our lab to pursue a master's.<br>
              <strong>Yehui Yang</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
              <strong>Ruiqi Ding</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
              <strong>Hanwen Liu</strong>: &ensp;&ensp; SJTU, gone to Technische Universität München, TUM to pursue a master's. <br>
              <strong>Xiaoyu Tian</strong>: &ensp;&ensp; SJTU, gone to Columbia University to pursue master. <br>
              <strong>Chi Zhang</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>             
              <strong>Huixin Zhang</strong>: &ensp;&ensp; SJTU, gone to our lab to pursue master. <br>
              <strong>Zehang Shen</strong>: &ensp;&ensp; SJTU, gone to our lab to pursue master. <br>
              <strong>Zhiheng Feng</strong>: &ensp;&ensp; SJTU, gone to our lab to pursue master. <br>
              <strong>Shuaiqi Ren</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
              <strong>Zike Cheng</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
              <strong>Jiquan Zhong</strong>: &ensp;&ensp; SJTU, gone to Xiamen University, XMU to pursue master. <br>
              <strong>Yanfeng Guo</strong>: &ensp;&ensp; SJTU, gone to University of California, Los Angeles, UCLA to pursue master. <br>
              <strong>Yunzhe Hu</strong>: &ensp;&ensp; SJTU, gone to The University of Hong Kong, HKU, to directly pursue a doctorate with a full scholarship. <br>
              <strong>Ziliang Wang</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue doctor. <br>
              <strong>Honghao Zeng</strong>: &ensp;&ensp; SJTU, gone to Shanghai Baosight Software Co., Ltd to work. <br>
              <strong>Muyao Chen</strong>: &ensp;&ensp; SJTU, gone to ByteDance to work. <br>
              <strong>Haolin Song</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
              <strong>Jiuming Liu</strong>: &ensp;&ensp; HIT, gone to our lab to pursue master. <br>
              <strong>Shuyang Jiang</strong>: &ensp;&ensp; SJTU, gone to Fudan University to pursue doctor. <br>
              <strong>Wenlong Yi</strong>: &ensp;&ensp; SJTU, gone to the University of California, Los Angeles, UCLA to pursue a master's. <br>
              <strong>Chensheng Peng</strong>: &ensp;&ensp; SJTU, gone to UC Berkeley to directly pursue a doctor with a full scholarship. <br> 
              <strong>Wenhua Wu</strong>: &ensp;&ensp; SJTU, gone to our lab to pursue doctor. <br>
              <strong>Yu Zheng</strong>: &ensp;&ensp; SJTU, gone to our lab to directly pursue a joint PhD in ETH and SJTU. <br>
              <strong>Jiahao Qiu</strong>: &ensp;&ensp; SJTU, gone to Princeton University to directly pursue a doctorate with a full scholarship. <br>
              <strong>Shijie Zhao</strong>: &ensp;&ensp; SJTU, gone to Georgia Institute of Technology, Gatech to pursue master. <br>
              
              <!-- <strong>Minjian Xin</strong>: &ensp;&ensp; SJTU, gone to <font color="red">University of California at San Diego, UCSD</font> to pursue master. <br>             
               <strong>Xinrui Wu</strong>: &ensp;&ensp; Shanghai Jiao Tong University, SJTU, gone to our lab to pursue a master's.<br>
               <strong>Yehui Yang</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
               <strong>Ruiqi Ding</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
               <strong>Hanwen Liu</strong>: &ensp;&ensp; SJTU, gone to <font color="red">Technische Universität München, TUM</font> to pursue master. <br>
               <strong>Xiaoyu Tian</strong>: &ensp;&ensp; SJTU, gone to <font color="red">Columbia University</font> to pursue master. <br>
               <strong>Chi Zhang</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>             
               <strong>Huixin Zhang</strong>: &ensp;&ensp; SJTU, gone to our lab to pursue master. <br>
               <strong>Zehang Shen</strong>: &ensp;&ensp; SJTU, gone to our lab to pursue master. <br>
               <strong>Zhiheng Feng</strong>: &ensp;&ensp; SJTU, gone to our lab to pursue master. <br>
               <strong>Shuaiqi Ren</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
               <strong>Zike Cheng</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
               <strong>Jiquan Zhong</strong>: &ensp;&ensp; SJTU, gone to Xiamen University, XMU to pursue master. <br>
               <strong>Yanfeng Guo</strong>: &ensp;&ensp; SJTU, gone to <font color="red">University of California, Los Angeles, UCLA</font> to pursue master. <br>
               <strong>Yunzhe Hu</strong>: &ensp;&ensp; SJTU, gone to <font color="red">The University of Hong Kong, HKU to directly pursue a doctor with full scholarship</font>. <br>
               <strong>Ziliang Wang</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue doctor. <br>
               <strong>Honghao Zeng</strong>: &ensp;&ensp; SJTU, gone to Shanghai Baosight Software Co., Ltd to work. <br>
               <strong>Muyao Chen</strong>: &ensp;&ensp; SJTU, gone to ByteDance to work. <br>
               <strong>Haolin Song</strong>: &ensp;&ensp; SJTU, gone to SJTU to pursue master. <br>
               <strong>Jiuming Liu</strong>: &ensp;&ensp; HIT, gone to our lab to pursue master. <br>
               <strong>Shuyang Jiang</strong>: &ensp;&ensp; SJTU, gone to Fudan University to pursue doctor. <br>
               <strong>Wenlong Yi</strong>: &ensp;&ensp; SJTU, gone to <font color="red">University of California, Los Angeles, UCLA</font> to pursue master. <br>
               <strong>Chensheng Peng</strong>: &ensp;&ensp; SJTU, gone to <font color="red">UC Berkeley to directly pursue doctor with full scholarship</font>. <br> 
               <strong>Wenhua Wu</strong>: &ensp;&ensp; SJTU, gone to our lab to pursue doctor. <br>
               <strong>Yu Zheng</strong>: &ensp;&ensp; SJTU, gone to our lab to directly pursue a joint PhD in ETH and SJTU. <br>
               <strong>Jiahao Qiu</strong>: &ensp;&ensp; SJTU, gone to <font color="red">Princeton University to directly pursue doctor with full scholarship</font>. <br>
               <strong>Shijie Zhao</strong>: &ensp;&ensp; SJTU, gone to <font color="red">Georgia Institute of Technology, Gatech</font> to pursue master. <br>
           -->
            </td>
       </tr></tbody>
    </table>


    <!--SECTION 8 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>About Me</heading>
           <p align="justify">In my free time, I like reading books on psychology and literature. I like travelling.
               I also enjoy sports and meditation.
		   <!--</br></br>-->
		   <!--<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> -->
		   </p>
		   </td></tr>
       </tbody>
    </table>


    <!--SECTION 9 -->
    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="100%" align="middle">
    <p align="center" style="width: 25% ">
    <script type="text/javascript" id="clstr_globe"  src="//cdn.clustrmaps.com/globe.js?d=1yPWWuOlXo22MrO9sRinBO9GUjLHe88Yk0lOK35nmQA"></script>
    </p></td>
    </tr>
    </tbody>
    </table> -->


    <!--SECTION 10 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td><br>
              <!--<p align="right"><font size="3">Erd&ouml;s = ? </font><br> -->
              <p align="right"><font size="2"> Last update: 2025.10. Thanks for <a href="http://www.cs.berkeley.edu/~barron/">Jon Barron's</a> website.</font></p>
           </td>
        </tr>
        </tbody>
    </table>


</td>
</tr>
</tbody>
</table>
</body>
</html>
